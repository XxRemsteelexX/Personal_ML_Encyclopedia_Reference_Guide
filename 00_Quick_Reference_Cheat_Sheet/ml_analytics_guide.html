<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML & Analytics Cheat Sheet Guide</title>
    <style>
        /* Print-ready setup: 8.5x11 inches at 96 DPI = 816x1056 pixels */
        @page {
            size: letter;
            margin: 0.25in;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.4;
            color: #2c3e50;
            background: white;
        }

        /* Page container */
        .page {
            width: 8.5in;
            height: 11in;
            padding: 0.5in;
            margin: 0 auto 0.5in;
            background: white;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            page-break-after: always;
            position: relative;
        }

        .page:last-child {
            page-break-after: auto;
        }

        /* Color Scheme */
        :root {
            --primary-blue: #2E5C8A;
            --teal: #1A7F8E;
            --orange: #E67E22;
            --green: #27AE60;
            --purple: #8E44AD;
            --red: #C0392B;
            --gray: #34495E;
            --light-gray: #ecf0f1;
            --light-blue: #E8F4F8;
            --light-green: #E8F6E8;
            --light-orange: #FFF3E8;
            --light-purple: #F4E8FF;
            --light-red: #FFE8E8;
        }

        /* Typography */
        h1 {
            font-size: 28pt;
            font-weight: 700;
            margin-bottom: 12pt;
            color: var(--primary-blue);
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        h2 {
            font-size: 20pt;
            font-weight: 700;
            margin-top: 16pt;
            margin-bottom: 10pt;
            color: var(--primary-blue);
            border-bottom: 3px solid var(--primary-blue);
            padding-bottom: 4pt;
        }

        h3 {
            font-size: 14pt;
            font-weight: 600;
            margin-top: 12pt;
            margin-bottom: 8pt;
            color: var(--teal);
        }

        h4 {
            font-size: 12pt;
            font-weight: 600;
            margin-top: 8pt;
            margin-bottom: 6pt;
            color: var(--gray);
        }

        p {
            font-size: 10pt;
            margin-bottom: 8pt;
        }

        /* Lists */
        ul, ol {
            margin-left: 20pt;
            margin-bottom: 10pt;
            font-size: 10pt;
        }

        li {
            margin-bottom: 4pt;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 12pt 0;
            font-size: 9pt;
        }

        th {
            background: var(--primary-blue);
            color: white;
            padding: 8pt;
            text-align: left;
            font-weight: 600;
            border: 1px solid #2c3e50;
        }

        td {
            padding: 6pt 8pt;
            border: 1px solid #ddd;
            vertical-align: top;
        }

        tr:nth-child(even) {
            background: var(--light-gray);
        }

        /* Info Boxes */
        .box {
            padding: 10pt;
            margin: 10pt 0;
            border-radius: 6pt;
            border-left: 4pt solid;
        }

        .tip {
            background: var(--light-green);
            border-left-color: var(--green);
        }

        .warning {
            background: var(--light-orange);
            border-left-color: var(--orange);
        }

        .danger {
            background: var(--light-red);
            border-left-color: var(--red);
        }

        .info {
            background: var(--light-blue);
            border-left-color: var(--teal);
        }

        /* Icons and Labels */
        .icon {
            display: inline-block;
            padding: 2pt 6pt;
            border-radius: 3pt;
            font-weight: 600;
            font-size: 9pt;
            margin-right: 4pt;
        }

        .icon-yes {
            background: var(--green);
            color: white;
        }

        .icon-no {
            background: var(--red);
            color: white;
        }

        .icon-maybe {
            background: var(--orange);
            color: white;
        }

        /* Decision Tree Styling */
        .decision-tree {
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            background: var(--light-gray);
            padding: 12pt;
            border-radius: 6pt;
            margin: 10pt 0;
            line-height: 1.6;
            overflow-x: auto;
        }

        /* Code blocks */
        code {
            background: #f8f9fa;
            padding: 2pt 4pt;
            border-radius: 3pt;
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            color: var(--red);
        }

        pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 10pt;
            border-radius: 6pt;
            margin: 10pt 0;
            font-size: 8pt;
            overflow-x: auto;
        }

        /* Grid layout for comparison */
        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 12pt;
            margin: 10pt 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 12pt;
            margin: 10pt 0;
        }

        /* Card styling */
        .card {
            background: white;
            border: 2px solid var(--light-gray);
            border-radius: 6pt;
            padding: 10pt;
        }

        .card h4 {
            margin-top: 0;
        }

        /* Page numbering */
        .page-number {
            position: absolute;
            bottom: 0.25in;
            right: 0.5in;
            font-size: 10pt;
            color: var(--gray);
        }

        /* Section badges */
        .badge {
            display: inline-block;
            padding: 4pt 8pt;
            border-radius: 12pt;
            font-size: 9pt;
            font-weight: 600;
            margin-right: 6pt;
            margin-bottom: 4pt;
        }

        .badge-supervised { background: var(--primary-blue); color: white; }
        .badge-unsupervised { background: var(--purple); color: white; }
        .badge-deep-learning { background: var(--orange); color: white; }
        .badge-nlp { background: var(--green); color: white; }
        .badge-stats { background: var(--red); color: white; }

        /* Flowchart elements */
        .flow-box {
            background: var(--light-blue);
            border: 2px solid var(--teal);
            padding: 8pt;
            margin: 6pt 0;
            border-radius: 4pt;
            text-align: center;
            font-weight: 600;
            font-size: 10pt;
        }

        .flow-arrow {
            text-align: center;
            font-size: 16pt;
            color: var(--teal);
            margin: 4pt 0;
        }

        /* Highlight */
        .highlight {
            background: yellow;
            padding: 1pt 3pt;
            font-weight: 600;
        }

        /* Print optimization */
        @media print {
            body {
                margin: 0;
                padding: 0;
            }
            
            .page {
                box-shadow: none;
                margin: 0;
            }
        }

        /* Strong emphasis */
        strong {
            color: var(--primary-blue);
            font-weight: 700;
        }

        /* Small text */
        .small {
            font-size: 8pt;
            color: #7f8c8d;
        }

        /* Center text */
        .center {
            text-align: center;
        }
    </style>
</head>
<body>

<!-- PAGE 1: COVER PAGE -->
<div class="page">
    <div style="height: 100%; display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center;">
        <div style="border: 4px solid var(--primary-blue); padding: 30pt; border-radius: 12pt; background: linear-gradient(135deg, var(--light-blue) 0%, white 100%);">
            <h1 style="font-size: 36pt; margin-bottom: 20pt; color: var(--primary-blue);">MACHINE LEARNING<br>& ANALYTICS</h1>
            <h2 style="font-size: 24pt; color: var(--teal); border: none; margin-bottom: 20pt;">Visual Cheat Sheet Guide</h2>
            <p style="font-size: 14pt; margin-bottom: 30pt; color: var(--gray);">Complete Reference for Model Selection, Feature Engineering,<br>Statistical Tests, and Best Practices</p>
            
            <div style="display: flex; gap: 12pt; justify-content: center; flex-wrap: wrap; margin-top: 30pt;">
                <span class="badge badge-supervised">SUPERVISED LEARNING</span>
                <span class="badge badge-unsupervised">UNSUPERVISED</span>
                <span class="badge badge-deep-learning">DEEP LEARNING</span>
                <span class="badge badge-nlp">NLP</span>
                <span class="badge badge-stats">STATISTICS</span>
            </div>
            
            <p style="margin-top: 40pt; font-size: 11pt; color: var(--gray);">
                <strong>Print-Ready ‚Ä¢ Laminate-Friendly ‚Ä¢ Color-Coded</strong><br>
                <span class="small">Updated October 2025</span>
            </p>
        </div>
    </div>
</div>

<!-- PAGE 2: TABLE OF CONTENTS -->
<div class="page">
    <h1>üìã Table of Contents</h1>
    
    <div style="columns: 2; column-gap: 20pt; font-size: 11pt;">
        <div style="break-inside: avoid; margin-bottom: 16pt;">
            <h3 style="color: var(--primary-blue);">üéØ Core Fundamentals</h3>
            <ul style="list-style: none; margin-left: 0;">
                <li><strong>Page 3:</strong> ML Workflow Overview</li>
                <li><strong>Page 4:</strong> Model Selection Master Tree</li>
                <li><strong>Page 5:</strong> Data Type ‚Üí Model Mapping</li>
            </ul>
        </div>
        
        <div style="break-inside: avoid; margin-bottom: 16pt;">
            <h3 style="color: var(--teal);">üîß Feature Engineering</h3>
            <ul style="list-style: none; margin-left: 0;">
                <li><strong>Page 6:</strong> Categorical Encoding Guide</li>
                <li><strong>Page 7:</strong> Numerical Transformations</li>
                <li><strong>Page 8:</strong> Feature Engineering Tips</li>
            </ul>
        </div>
        
        <div style="break-inside: avoid; margin-bottom: 16pt;">
            <h3 style="color: var(--red);">üìä Statistical Tests</h3>
            <ul style="list-style: none; margin-left: 0;">
                <li><strong>Page 9:</strong> Test Selection Flowchart</li>
                <li><strong>Page 10:</strong> A/B Testing Guide</li>
            </ul>
        </div>
        
        <div style="break-inside: avoid; margin-bottom: 16pt;">
            <h3 style="color: var(--orange);">üß† Deep Learning</h3>
            <ul style="list-style: none; margin-left: 0;">
                <li><strong>Page 11:</strong> When to Use DL</li>
                <li><strong>Page 12:</strong> Architecture Selection</li>
                <li><strong>Page 13:</strong> Training Best Practices</li>
            </ul>
        </div>
        
        <div style="break-inside: avoid; margin-bottom: 16pt;">
            <h3 style="color: var(--green);">üìù NLP & Transformers</h3>
            <ul style="list-style: none; margin-left: 0;">
                <li><strong>Page 14:</strong> NLP Task Selection</li>
                <li><strong>Page 15:</strong> Transformer Quick Ref</li>
            </ul>
        </div>
        
        <div style="break-inside: avoid; margin-bottom: 16pt;">
            <h3 style="color: var(--purple);">üìà Evaluation & Metrics</h3>
            <ul style="list-style: none; margin-left: 0;">
                <li><strong>Page 16:</strong> Classification Metrics</li>
                <li><strong>Page 17:</strong> Regression Metrics</li>
            </ul>
        </div>
        
        <div style="break-inside: avoid; margin-bottom: 16pt;">
            <h3 style="color: var(--gray);">‚öôÔ∏è Reference Tables</h3>
            <ul style="list-style: none; margin-left: 0;">
                <li><strong>Page 18:</strong> Model Comparison Matrix</li>
                <li><strong>Page 19:</strong> Hyperparameter Guide</li>
                <li><strong>Page 20:</strong> Common Algorithms Code</li>
            </ul>
        </div>
    </div>
    
    <div class="box info" style="margin-top: 20pt;">
        <h4>üéØ How to Use This Guide</h4>
        <ul style="font-size: 10pt;">
            <li><strong>Color Coding:</strong> Each topic has a distinct color for quick navigation</li>
            <li><strong>Decision Trees:</strong> Follow flowcharts for model selection</li>
            <li><strong>Comparison Tables:</strong> Side-by-side comparison of techniques</li>
            <li><strong>Practical Tips:</strong> Colored boxes highlight best practices and warnings</li>
            <li><strong>Cross-References:</strong> Page numbers link related topics</li>
        </ul>
    </div>
    
    <div class="box tip" style="margin-top: 12pt;">
        <h4>‚úÖ Best Practices</h4>
        <p><strong>Start Simple:</strong> Always baseline with simple models (Logistic Regression, Random Forest) before complex ones.</p>
        <p><strong>XGBoost Usually Wins:</strong> For tabular data, XGBoost/LightGBM beats neural networks 90% of the time.</p>
        <p><strong>Cross-Validate Everything:</strong> Never rely on a single train/test split.</p>
    </div>
    
    <div class="page-number">Page 2</div>
</div>

<!-- PAGE 3: ML WORKFLOW OVERVIEW -->
<div class="page">
    <h1>üîÑ Machine Learning Workflow</h1>
    
    <div class="flow-box" style="background: var(--light-blue); border-color: var(--primary-blue); font-size: 11pt;">
        1Ô∏è‚É£ UNDERSTAND THE PROBLEM
    </div>
    <div class="flow-arrow">‚Üì</div>
    <p style="font-size: 10pt; padding-left: 12pt;">
        ‚Ä¢ What are you predicting? (Classification / Regression / Generation)<br>
        ‚Ä¢ What's the success metric? (Accuracy / F1 / RMSE / Business KPI)<br>
        ‚Ä¢ What constraints exist? (Time / Compute / Interpretability)
    </p>
    
    <div class="flow-box" style="background: var(--light-green); border-color: var(--green);">
        2Ô∏è‚É£ EXPLORE YOUR DATA (EDA)
    </div>
    <div class="flow-arrow">‚Üì</div>
    <p style="font-size: 10pt; padding-left: 12pt;">
        ‚Ä¢ Sample size? (100 / 1K / 10K / 100K / 1M+)<br>
        ‚Ä¢ Feature count? (10 / 100 / 1000+)<br>
        ‚Ä¢ Data types? (Numerical / Categorical / Text / Images)<br>
        ‚Ä¢ Missing values? Outliers? Class imbalance?
    </p>
    
    <div class="flow-box" style="background: var(--light-purple); border-color: var(--purple);">
        3Ô∏è‚É£ FEATURE ENGINEERING
    </div>
    <div class="flow-arrow">‚Üì</div>
    <div class="grid-2" style="font-size: 9pt;">
        <div>
            <h4 style="color: var(--teal);">Categorical Features</h4>
            ‚Ä¢ Tree models ‚Üí Label/Target encoding<br>
            ‚Ä¢ Linear models ‚Üí One-hot encoding
        </div>
        <div>
            <h4 style="color: var(--teal);">Numerical Features</h4>
            ‚Ä¢ Linear models ‚Üí Scale (StandardScaler)<br>
            ‚Ä¢ Tree models ‚Üí No scaling needed
        </div>
    </div>
    
    <div class="flow-box" style="background: var(--light-orange); border-color: var(--orange);">
        4Ô∏è‚É£ MODEL SELECTION ‚Üí See Page 4
    </div>
    <div class="flow-arrow">‚Üì</div>
    
    <div class="grid-2" style="font-size: 9pt; margin-bottom: 8pt;">
        <div>
            <h4>Based on Data Type</h4>
            ‚Ä¢ Tabular ‚Üí XGBoost/RF<br>
            ‚Ä¢ Images ‚Üí CNN<br>
            ‚Ä¢ Text ‚Üí Transformers<br>
            ‚Ä¢ Time Series ‚Üí ARIMA/LSTM
        </div>
        <div>
            <h4>Based on Sample Size</h4>
            ‚Ä¢ <1K ‚Üí Simple models<br>
            ‚Ä¢ 1K-10K ‚Üí XGBoost<br>
            ‚Ä¢ 10K+ ‚Üí DL possible<br>
            ‚Ä¢ 100K+ ‚Üí DL recommended
        </div>
    </div>
    
    <div class="flow-box" style="background: #FFE8E8; border-color: var(--red);">
        5Ô∏è‚É£ TRAINING & VALIDATION
    </div>
    <div class="flow-arrow">‚Üì</div>
    <p style="font-size: 10pt; padding-left: 12pt;">
        ‚Ä¢ Split: 70-80% train, 10-15% validation, 10-15% test<br>
        ‚Ä¢ Use stratified split for imbalanced data<br>
        ‚Ä¢ Cross-validation (5-fold minimum)<br>
        ‚Ä¢ Hyperparameter tuning on validation set
    </p>
    
    <div class="flow-box" style="background: var(--light-blue); border-color: var(--teal);">
        6Ô∏è‚É£ EVALUATION ‚Üí See Pages 16-17
    </div>
    <div class="flow-arrow">‚Üì</div>
    <div class="grid-2" style="font-size: 9pt;">
        <div>
            <h4>Classification</h4>
            ‚Ä¢ Balanced ‚Üí Accuracy<br>
            ‚Ä¢ Imbalanced ‚Üí F1, PR-AUC<br>
            ‚Ä¢ Cost-sensitive ‚Üí Precision/Recall
        </div>
        <div>
            <h4>Regression</h4>
            ‚Ä¢ General ‚Üí RMSE, MAE<br>
            ‚Ä¢ % variance ‚Üí R¬≤<br>
            ‚Ä¢ Relative errors ‚Üí MAPE
        </div>
    </div>
    
    <div class="flow-box" style="background: var(--light-green); border-color: var(--green);">
        7Ô∏è‚É£ DEPLOYMENT & MONITORING
    </div>
    
    <div class="box danger" style="margin-top: 12pt;">
        <h4>‚ö†Ô∏è Common Pitfalls to Avoid</h4>
        <ul style="font-size: 9pt;">
            <li><strong>Data Leakage:</strong> Never use test data during training or feature engineering</li>
            <li><strong>Overfitting:</strong> Monitor train vs validation performance gap</li>
            <li><strong>Wrong Metric:</strong> Don't use accuracy for imbalanced datasets</li>
            <li><strong>No Baseline:</strong> Always compare against simple model first</li>
            <li><strong>Ignoring Domain:</strong> Feature engineering needs domain knowledge</li>
        </ul>
    </div>
    
    <div class="box tip" style="margin-top: 8pt;">
        <h4>‚úÖ Golden Rules</h4>
        <p style="font-size: 9pt;"><strong>1. Start Simple:</strong> Baseline first (Logistic Reg / Random Forest)<br>
        <strong>2. Validate Properly:</strong> Always use cross-validation<br>
        <strong>3. Test Significance:</strong> Ensure improvements are statistically significant<br>
        <strong>4. Document Everything:</strong> Track experiments and hyperparameters</p>
    </div>
    
    <div class="page-number">Page 3</div>
</div>

<!-- PAGE 4: MODEL SELECTION MASTER TREE -->
<div class="page">
    <h1>üéØ Model Selection Master Decision Tree</h1>
    
    <h2>Step 1: What Type of Data?</h2>
    
    <h3 style="color: var(--primary-blue); margin-top: 12pt;">üìä TABULAR / STRUCTURED DATA (Most Common)</h3>
    <table style="font-size: 9pt; margin-bottom: 12pt;">
        <tr>
            <th style="width: 25%;">Sample Size</th>
            <th style="width: 40%;">Recommended Models</th>
            <th style="width: 35%;">Why</th>
        </tr>
        <tr>
            <td><strong>&lt; 1,000 samples</strong></td>
            <td>Logistic Regression OR Random Forest</td>
            <td>Simple models generalize better with limited data</td>
        </tr>
        <tr>
            <td><strong>1K - 10K</strong></td>
            <td><strong>XGBoost (best)</strong> OR Random Forest</td>
            <td>XGBoost handles mixed types, missing values excellently</td>
        </tr>
        <tr>
            <td><strong>10K - 100K</strong></td>
            <td><strong>XGBoost / LightGBM</strong> (winner 90%)</td>
            <td>Beats neural networks on tabular data consistently</td>
        </tr>
        <tr>
            <td><strong>100K+</strong></td>
            <td><strong>LightGBM (fastest)</strong> OR Deep Learning</td>
            <td>LightGBM optimized for large datasets; DL if complex interactions</td>
        </tr>
    </table>

    <h3 style="color: var(--orange); margin-top: 12pt;">üñºÔ∏è IMAGES</h3>
    <table style="font-size: 9pt; margin-bottom: 12pt;">
        <tr>
            <th style="width: 25%;">Dataset Size</th>
            <th style="width: 40%;">Recommended Approach</th>
            <th style="width: 35%;">Models</th>
        </tr>
        <tr>
            <td><strong>&lt; 10K images</strong></td>
            <td><strong>Transfer Learning</strong></td>
            <td>ResNet, EfficientNet, Vision Transformers</td>
        </tr>
        <tr>
            <td><strong>10K - 100K</strong></td>
            <td>Fine-tune pretrained CNN</td>
            <td>ResNet50, EfficientNet-B0, MobileNet</td>
        </tr>
        <tr>
            <td><strong>100K+</strong></td>
            <td>Train from scratch OR fine-tune</td>
            <td>EfficientNet-B7, Custom CNNs, ViT</td>
        </tr>
    </table>

    <h3 style="color: var(--green); margin-top: 12pt;">üìù TEXT / NLP</h3>
    <table style="font-size: 9pt; margin-bottom: 12pt;">
        <tr>
            <th style="width: 25%;">Sample Size</th>
            <th style="width: 40%;">Recommended Models</th>
            <th style="width: 35%;">Notes</th>
        </tr>
        <tr>
            <td><strong>&lt; 1K samples</strong></td>
            <td>TF-IDF + Logistic Reg OR Few-shot LLM</td>
            <td>Simple baseline works well; LLMs for complex tasks</td>
        </tr>
        <tr>
            <td><strong>1K - 100K</strong></td>
            <td><strong>Fine-tune BERT/RoBERTa</strong></td>
            <td>Pretrained language models + task-specific tuning</td>
        </tr>
        <tr>
            <td><strong>100K+</strong></td>
            <td>Fine-tune large Transformer (GPT, T5)</td>
            <td>Enough data for larger models</td>
        </tr>
    </table>

    <h3 style="color: var(--purple); margin-top: 12pt;">üìà TIME SERIES</h3>
    <table style="font-size: 9pt; margin-bottom: 12pt;">
        <tr>
            <th style="width: 25%;">Data Points</th>
            <th style="width: 40%;">Recommended Models</th>
            <th style="width: 35%;">Why</th>
        </tr>
        <tr>
            <td><strong>&lt; 100 points</strong></td>
            <td>ARIMA / Exponential Smoothing</td>
            <td>Statistical methods work well with limited data</td>
        </tr>
        <tr>
            <td><strong>100 - 10K</strong></td>
            <td>Prophet OR SARIMA</td>
            <td>Prophet good for business data with seasonality</td>
        </tr>
        <tr>
            <td><strong>10K+</strong></td>
            <td>LSTM OR XGBoost with lag features</td>
            <td>Deep learning for complex patterns; XGBoost often competitive</td>
        </tr>
    </table>

    <h3 style="color: var(--red); margin-top: 12pt;">üéµ AUDIO / VIDEO</h3>
    <div class="box info" style="font-size: 9pt;">
        <p><strong>General Approach:</strong></p>
        <ol style="margin-left: 20pt;">
            <li>Extract features using pretrained models</li>
            <li>Process with CNN (spatial) + RNN (temporal)</li>
            <li>Use specialized architectures: Wav2Vec (audio), Whisper (speech), 3D CNNs (video)</li>
        </ol>
    </div>
    
    <h2>Step 2: Special Requirements?</h2>
    
    <table style="font-size: 9pt;">
        <tr>
            <th style="width: 25%;">Requirement</th>
            <th style="width: 35%;">Best Choice</th>
            <th style="width: 40%;">Why</th>
        </tr>
        <tr>
            <td><strong>Interpretability</strong></td>
            <td>Logistic Regression<br>Decision Tree</td>
            <td>Clear coefficients and rules</td>
        </tr>
        <tr>
            <td><strong>Fast Inference (&lt;1ms)</strong></td>
            <td>Logistic Regression<br>Small Decision Tree</td>
            <td>Minimal computation, no ensemble overhead</td>
        </tr>
        <tr>
            <td><strong>Fast Training</strong></td>
            <td>Logistic Regression<br>Random Forest</td>
            <td>Less tuning needed, parallelizable</td>
        </tr>
        <tr>
            <td><strong>Class Imbalance</strong></td>
            <td>XGBoost<br>Random Forest</td>
            <td>Handle with scale_pos_weight, class_weight</td>
        </tr>
        <tr>
            <td><strong>High Dimensionality (1000+ features)</strong></td>
            <td>L1 Logistic Regression<br>XGBoost</td>
            <td>Built-in feature selection</td>
        </tr>
        <tr>
            <td><strong>Missing Values</strong></td>
            <td>XGBoost<br>LightGBM</td>
            <td>Handle missing natively</td>
        </tr>
        <tr>
            <td><strong>No GPU Available</strong></td>
            <td>LightGBM<br>Random Forest</td>
            <td>CPU-optimized, fast</td>
        </tr>
        <tr>
            <td><strong>Need Probabilities</strong></td>
            <td>Logistic Regression<br>Calibrated classifiers</td>
            <td>Well-calibrated by default</td>
        </tr>
    </table>
    
    <div class="box warning" style="margin-top: 10pt;">
        <h4>‚ö†Ô∏è Common Mistakes in Model Selection</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>‚ùå Using Deep Learning for Tabular Data</strong><br>
                XGBoost typically outperforms neural nets on structured data.</p>
                <p><strong>‚ùå One-Hot Encoding with Tree Models</strong><br>
                Use Label or Target encoding instead.</p>
            </div>
            <div>
                <p><strong>‚ùå Not Starting with Baseline</strong><br>
                Always try simple model first (Logistic Reg).</p>
                <p><strong>‚ùå Ignoring Sample Size</strong><br>
                Deep learning needs 100K+ samples to shine.</p>
            </div>
        </div>
    </div>
    
    <div class="box tip" style="margin-top: 8pt;">
        <h4>‚úÖ Recommended Starting Points</h4>
        <div class="grid-3" style="font-size: 9pt;">
            <div>
                <strong style="color: var(--primary-blue);">Small Data (&lt;1K)</strong><br>
                1. Logistic Regression<br>
                2. Random Forest<br>
                3. SVM (if &lt;10K samples)
            </div>
            <div>
                <strong style="color: var(--teal);">Medium Data (1K-100K)</strong><br>
                1. XGBoost (1st choice)<br>
                2. LightGBM<br>
                3. Random Forest
            </div>
            <div>
                <strong style="color: var(--orange);">Large Data (100K+)</strong><br>
                1. LightGBM (fastest)<br>
                2. XGBoost<br>
                3. Deep Learning (if complex)
            </div>
        </div>
    </div>
    
    <div class="page-number">Page 4</div>
</div>

<!-- PAGE 5: DATA TYPE TO MODEL MAPPING -->
<div class="page">
    <h1>üóÇÔ∏è Data Type ‚Üí Model Mapping</h1>
    
    <h2>Quick Reference Table</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th>Problem Type</th>
            <th>Data Type</th>
            <th>Sample Size</th>
            <th>Best Model</th>
            <th>Alternative</th>
            <th>Avoid</th>
        </tr>
        <tr style="background: var(--light-blue);">
            <td rowspan="3"><strong>Classification</strong></td>
            <td>Tabular</td>
            <td>&lt;1K</td>
            <td>Random Forest</td>
            <td>Logistic Reg</td>
            <td>Deep Learning</td>
        </tr>
        <tr>
            <td>Tabular</td>
            <td>1K-100K</td>
            <td><strong>XGBoost</strong></td>
            <td>LightGBM</td>
            <td>One-hot + Trees</td>
        </tr>
        <tr style="background: var(--light-blue);">
            <td>Tabular</td>
            <td>100K+</td>
            <td><strong>LightGBM</strong></td>
            <td>XGBoost</td>
            <td>-</td>
        </tr>
        <tr style="background: var(--light-green);">
            <td rowspan="2"><strong>Regression</strong></td>
            <td>Tabular (Linear)</td>
            <td>Any</td>
            <td>Ridge Regression</td>
            <td>Lasso</td>
            <td>-</td>
        </tr>
        <tr>
            <td>Tabular (Non-linear)</td>
            <td>1K+</td>
            <td><strong>XGBoost</strong></td>
            <td>Random Forest</td>
            <td>-</td>
        </tr>
        <tr style="background: var(--light-orange);">
            <td rowspan="2"><strong>Images</strong></td>
            <td>Classification</td>
            <td>&lt;10K</td>
            <td>Transfer Learning</td>
            <td>ResNet/EfficientNet</td>
            <td>Train from scratch</td>
        </tr>
        <tr>
            <td>Classification</td>
            <td>100K+</td>
            <td><strong>Fine-tune CNN</strong></td>
            <td>Train from scratch</td>
            <td>-</td>
        </tr>
        <tr style="background: var(--light-purple);">
            <td rowspan="3"><strong>Text / NLP</strong></td>
            <td>Classification</td>
            <td>&lt;1K</td>
            <td>TF-IDF + LogReg</td>
            <td>Few-shot GPT</td>
            <td>BERT (too little data)</td>
        </tr>
        <tr>
            <td>Classification</td>
            <td>1K-10K</td>
            <td><strong>Fine-tune BERT</strong></td>
            <td>RoBERTa</td>
            <td>-</td>
        </tr>
        <tr style="background: var(--light-purple);">
            <td>Generation</td>
            <td>Any</td>
            <td>GPT-4 / GPT-3.5</td>
            <td>Llama 3 / Mistral</td>
            <td>Train from scratch</td>
        </tr>
        <tr>
            <td rowspan="3"><strong>Time Series</strong></td>
            <td>Univariate</td>
            <td>&lt;100</td>
            <td>ARIMA</td>
            <td>Exp. Smoothing</td>
            <td>Deep Learning</td>
        </tr>
        <tr style="background: var(--light-blue);">
            <td>Univariate</td>
            <td>100-10K</td>
            <td><strong>Prophet</strong></td>
            <td>SARIMA</td>
            <td>-</td>
        </tr>
        <tr>
            <td>Multivariate</td>
            <td>10K+</td>
            <td><strong>LSTM</strong></td>
            <td>XGBoost + lags</td>
            <td>-</td>
        </tr>
    </table>
    
    <h2>Problem-Specific Recommendations</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">üìä E-Commerce / Business</h4>
            <p><strong>Customer Churn:</strong> XGBoost<br>
            <strong>Product Recommendations:</strong> Collaborative Filtering<br>
            <strong>Price Prediction:</strong> XGBoost / LightGBM<br>
            <strong>Sentiment Analysis:</strong> BERT</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--green);">üè• Healthcare / Life Sciences</h4>
            <p><strong>Disease Prediction:</strong> Random Forest (interpretable)<br>
            <strong>Medical Image Analysis:</strong> CNN (ResNet)<br>
            <strong>Clinical Text:</strong> BioBERT<br>
            <strong>Drug Discovery:</strong> Graph Neural Networks</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">üí∞ Finance / Risk</h4>
            <p><strong>Fraud Detection:</strong> XGBoost + SMOTE<br>
            <strong>Credit Scoring:</strong> Logistic Reg (interpretable)<br>
            <strong>Stock Prediction:</strong> LSTM / XGBoost<br>
            <strong>Anomaly Detection:</strong> Isolation Forest</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--purple);">üéØ Marketing / Growth</h4>
            <p><strong>Customer Segmentation:</strong> K-Means<br>
            <strong>LTV Prediction:</strong> XGBoost<br>
            <strong>A/B Test Analysis:</strong> Statistical Tests (p9-10)<br>
            <strong>Ad Click Prediction:</strong> XGBoost</p>
        </div>
    </div>
    
    <div class="box info" style="margin-top: 10pt;">
        <h4>üéØ Decision Framework</h4>
        <p style="font-size: 9pt;"><strong>Step 1:</strong> Identify data type (tabular, text, images)<br>
        <strong>Step 2:</strong> Check sample size (&lt;1K, 1K-10K, 10K-100K, 100K+)<br>
        <strong>Step 3:</strong> Consider constraints (interpretability, speed, compute)<br>
        <strong>Step 4:</strong> Select from table above<br>
        <strong>Step 5:</strong> Start simple, iterate to complex</p>
    </div>
    
    <div class="page-number">Page 5</div>
</div>

<!-- PAGE 6: CATEGORICAL ENCODING GUIDE -->
<div class="page">
    <h1>üè∑Ô∏è Categorical Encoding Guide</h1>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è CRITICAL: Encoding Choice Dramatically Affects Performance!</h4>
        <p style="font-size: 9pt;">Wrong encoding can reduce accuracy by 10-20%. <strong>Tree models MUST NOT use one-hot encoding.</strong></p>
    </div>
    
    <h2>Encoding Selection by Model Type</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th>Encoding</th>
            <th>Tree Models<br>(RF, XGBoost)</th>
            <th>Linear Models<br>(LogReg, SVM)</th>
            <th>Neural Nets</th>
            <th>Cardinality</th>
            <th>When to Use</th>
        </tr>
        <tr>
            <td><strong>Label</strong></td>
            <td><span class="icon icon-yes">‚úì Excellent</span></td>
            <td><span class="icon icon-no">‚úó Bad</span></td>
            <td><span class="icon icon-maybe">‚ñ≥ OK</span></td>
            <td>&lt; 50</td>
            <td>Default for trees, fast, no dimensions added</td>
        </tr>
        <tr>
            <td><strong>One-Hot</strong></td>
            <td><span class="icon icon-no">‚úó Bad</span></td>
            <td><span class="icon icon-yes">‚úì Required</span></td>
            <td><span class="icon icon-maybe">‚ñ≥ OK</span></td>
            <td>&lt; 10</td>
            <td>Linear models with low cardinality</td>
        </tr>
        <tr>
            <td><strong>Target</strong></td>
            <td><span class="icon icon-yes">‚úì‚úì Best</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td>50+</td>
            <td>High cardinality, use with CV to prevent leakage</td>
        </tr>
        <tr>
            <td><strong>Frequency</strong></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td>Any</td>
            <td>Safe, no leakage, simple</td>
        </tr>
        <tr>
            <td><strong>Binary</strong></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td>10-100</td>
            <td>Balance between one-hot and label</td>
        </tr>
        <tr>
            <td><strong>Ordinal</strong></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td>Any</td>
            <td>Natural ordering exists (Small/Med/Large)</td>
        </tr>
        <tr>
            <td><strong>Hash</strong></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td><span class="icon icon-maybe">‚ñ≥ OK</span></td>
            <td><span class="icon icon-yes">‚úì Good</span></td>
            <td>1000+</td>
            <td>Very high cardinality, memory constrained</td>
        </tr>
        <tr>
            <td><strong>Embedding</strong></td>
            <td><span class="icon icon-no">‚úó N/A</span></td>
            <td><span class="icon icon-no">‚úó N/A</span></td>
            <td><span class="icon icon-yes">‚úì‚úì Best</span></td>
            <td>Any</td>
            <td>Neural nets, learns representations</td>
        </tr>
    </table>
    
    <h2>Decision Tree: Which Encoding?</h2>
    
    <div class="decision-tree">
What model are you using?

TREE-BASED (Random Forest, XGBoost, LightGBM)
‚îÇ
‚îú‚îÄ Cardinality &lt; 50?
‚îÇ  ‚îî‚îÄ Use: <strong>Label Encoding</strong> (default)
‚îÇ
‚îî‚îÄ Cardinality ‚â• 50 (high)?
   ‚îú‚îÄ Use: <strong>Target Encoding</strong> (with CV!)
   ‚îî‚îÄ Alternative: Frequency Encoding
   
‚ùå NEVER use One-Hot with trees (slower, worse performance)

LINEAR MODELS (Logistic, Linear Reg, SVM)
‚îÇ
‚îú‚îÄ Cardinality &lt; 10?
‚îÇ  ‚îî‚îÄ Use: <strong>One-Hot Encoding</strong>
‚îÇ
‚îú‚îÄ Cardinality 10-50?
‚îÇ  ‚îú‚îÄ Use: Target Encoding
‚îÇ  ‚îî‚îÄ OR: One-Hot + dimensionality reduction
‚îÇ
‚îî‚îÄ Cardinality &gt; 50?
   ‚îî‚îÄ Use: <strong>Target Encoding</strong> OR Hash

NEURAL NETWORKS
‚îÇ
‚îî‚îÄ Use: <strong>Embedding Layer</strong> (learns best representation)
    </div>
    
    <div class="box danger">
        <h4>üö´ Target Encoding Leakage Prevention</h4>
        <pre style="font-size: 8pt; margin: 6pt 0;">
# ‚ùå WRONG - Leakage!
df['city_encoded'] = df.groupby('city')['target'].transform('mean')
X_train, X_test = train_test_split(df)

# ‚úÖ CORRECT - Split first, then encode
X_train, X_test, y_train, y_test = train_test_split(X, y)
encoder = TargetEncoder(smoothing=1.0)
X_train_enc = encoder.fit_transform(X_train, y_train)
X_test_enc = encoder.transform(X_test)  # No fit!
        </pre>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ Encoding Best Practices</h4>
        <ul style="font-size: 9pt;">
            <li><strong>Always split before encoding</strong> to prevent leakage</li>
            <li><strong>Use smoothing</strong> in target encoding (blend with global mean)</li>
            <li><strong>Save encoders</strong> to apply same transformation at inference</li>
            <li><strong>Check cardinality first:</strong> <code>df['col'].nunique()</code></li>
        </ul>
    </div>
    
    <div class="page-number">Page 6</div>
</div>

<!-- PAGE 7: NUMERICAL TRANSFORMATIONS -->
<div class="page">
    <h1>üìê Numerical Transformations</h1>
    
    <h2>Scaling: When and How?</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th style="width: 25%;">Model Type</th>
            <th style="width: 20%;">Scaling Needed?</th>
            <th style="width: 25%;">Best Scaler</th>
            <th style="width: 30%;">Why</th>
        </tr>
        <tr>
            <td><strong>Tree-Based</strong><br>(RF, XGBoost, DT)</td>
            <td><span class="icon icon-no">‚úó NO</span></td>
            <td>None needed</td>
            <td>Trees split on thresholds, scale doesn't matter</td>
        </tr>
        <tr>
            <td><strong>Linear Models</strong><br>(LogReg, Linear Reg)</td>
            <td><span class="icon icon-yes">‚úì YES</span></td>
            <td>StandardScaler</td>
            <td>Coefficients depend on scale, gradient descent sensitive</td>
        </tr>
        <tr>
            <td><strong>SVM</strong></td>
            <td><span class="icon icon-yes">‚úì YES</span></td>
            <td>StandardScaler</td>
            <td>Distance-based, very sensitive to scale</td>
        </tr>
        <tr>
            <td><strong>Neural Networks</strong></td>
            <td><span class="icon icon-yes">‚úì YES</span></td>
            <td>StandardScaler<br>OR MinMaxScaler</td>
            <td>Helps training stability and convergence</td>
        </tr>
        <tr>
            <td><strong>K-NN, K-Means</strong></td>
            <td><span class="icon icon-yes">‚úì CRITICAL</span></td>
            <td>StandardScaler</td>
            <td>Distance-based, large features dominate otherwise</td>
        </tr>
        <tr>
            <td><strong>Naive Bayes</strong></td>
            <td><span class="icon icon-no">‚úó NO</span></td>
            <td>None needed</td>
            <td>Works on probabilities, not distances</td>
        </tr>
    </table>
    
    <h2>Scaler Types</h2>
    
    <div class="grid-3" style="font-size: 9pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">StandardScaler</h4>
            <p style="font-size: 8.5pt;"><strong>Formula:</strong> z = (x - Œº) / œÉ<br>
            <strong>Result:</strong> mean=0, std=1<br>
            <strong>Use for:</strong> Default choice for linear/neural<br>
            <strong>Sensitive to:</strong> Outliers</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--teal);">MinMaxScaler</h4>
            <p style="font-size: 8.5pt;"><strong>Formula:</strong> x' = (x - min) / (max - min)<br>
            <strong>Result:</strong> range [0, 1]<br>
            <strong>Use for:</strong> Neural nets with sigmoid, images<br>
            <strong>Sensitive to:</strong> Outliers (very)</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">RobustScaler</h4>
            <p style="font-size: 8.5pt;"><strong>Formula:</strong> x' = (x - median) / IQR<br>
            <strong>Result:</strong> median=0<br>
            <strong>Use for:</strong> Data with outliers<br>
            <strong>Robust to:</strong> Outliers ‚úì</p>
        </div>
    </div>
    
    <h2>Distribution Transformations</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div>
            <h4 style="color: var(--purple);">Log Transform</h4>
            <p><strong>When:</strong> Right-skewed data (long tail)<br>
            <strong>Formula:</strong> log(x) or log(x+1) for zeros<br>
            <strong>Best for:</strong> Linear models<br>
            <strong>Example:</strong> Income, prices, counts</p>
            <pre style="font-size: 8pt;">
# For positive values
df['log_income'] = np.log(df['income'])

# For values with zeros
df['log_sales'] = np.log1p(df['sales'])
            </pre>
        </div>
        
        <div>
            <h4 style="color: var(--green);">Square Root</h4>
            <p><strong>When:</strong> Moderate skewness<br>
            <strong>Formula:</strong> ‚àöx<br>
            <strong>Best for:</strong> Count data<br>
            <strong>Less aggressive</strong> than log</p>
            <pre style="font-size: 8pt;">
df['sqrt_count'] = np.sqrt(df['count'])

# Box-Cox (optimal transform)
from scipy.stats import boxcox
df['transformed'], lambda = boxcox(df['col'])
            </pre>
        </div>
    </div>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è Common Scaling Pitfalls</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>‚ùå Fitting on full data (leakage)</strong></p>
                <pre style="font-size: 7.5pt;">
# WRONG
scaler.fit(pd.concat([X_train, X_test]))

# CORRECT
scaler.fit(X_train)  # Only train!
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
                </pre>
            </div>
            <div>
                <p><strong>‚ùå Scaling tree-based models</strong></p>
                <pre style="font-size: 7.5pt;">
# WRONG - Wastes time, no benefit
scaler.fit_transform(X)
XGBClassifier().fit(X_scaled, y)

# CORRECT - Skip scaling for trees
XGBClassifier().fit(X, y)
                </pre>
            </div>
        </div>
    </div>
    
    <h2>Quick Reference: When to Transform</h2>
    
    <table style="font-size: 8.5pt; margin-top: 8pt;">
        <tr>
            <th>Data Characteristic</th>
            <th>Transformation</th>
            <th>Models That Benefit</th>
        </tr>
        <tr>
            <td>Right-skewed (long tail)</td>
            <td>Log or Square Root</td>
            <td>Linear, Neural Nets</td>
        </tr>
        <tr>
            <td>Different scales (age vs income)</td>
            <td>StandardScaler</td>
            <td>Linear, SVM, K-NN, Neural Nets</td>
        </tr>
        <tr>
            <td>Outliers present</td>
            <td>RobustScaler OR Log</td>
            <td>Linear, SVM</td>
        </tr>
        <tr>
            <td>Need [0,1] range</td>
            <td>MinMaxScaler</td>
            <td>Neural Nets with sigmoid</td>
        </tr>
        <tr>
            <td>Normally distributed already</td>
            <td>StandardScaler (optional)</td>
            <td>Linear models</td>
        </tr>
        <tr>
            <td>Tree-based model</td>
            <td><strong>None!</strong></td>
            <td>RF, XGBoost, LightGBM</td>
        </tr>
    </table>
    
    <div class="box tip">
        <h4>‚úÖ Transformation Best Practices</h4>
        <p style="font-size: 9pt;"><strong>1. Visualize first:</strong> Use histograms to check skewness<br>
        <strong>2. Transform before scaling:</strong> Log ‚Üí then StandardScaler<br>
        <strong>3. Test both:</strong> Compare with/without transformation<br>
        <strong>4. Trees rarely need transformations</strong></p>
    </div>
    
    <div class="page-number">Page 7</div>
</div>

<!-- PAGE 8: FEATURE ENGINEERING TIPS -->
<div class="page">
    <h1>üîß Feature Engineering Tips</h1>
    
    <h2>DateTime Features</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">Basic Extraction</h4>
            <pre style="font-size: 7.5pt;">
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['hour'] = df['date'].dt.hour
df['day_of_week'] = df['date'].dt.dayofweek
df['quarter'] = df['date'].dt.quarter
df['week_of_year'] = df['date'].dt.isocalendar().week
            </pre>
        </div>
        
        <div class="card">
            <h4 style="color: var(--teal);">Binary Indicators</h4>
            <pre style="font-size: 7.5pt;">
# Weekend
df['is_weekend'] = df['day_of_week'].isin([5,6])

# Business hours
df['is_business_hours'] = df['hour'].between(9,17)

# Month end
df['is_month_end'] = df['date'].dt.is_month_end

# Holiday
import holidays
us_holidays = holidays.US()
df['is_holiday'] = df['date'].dt.date.isin(us_holidays)
            </pre>
        </div>
    </div>
    
    <div class="box info">
        <h4>üîÑ Cyclical Encoding (for Linear Models & Neural Nets)</h4>
        <p style="font-size: 9pt;"><strong>Why?</strong> Hour 23 is close to hour 0, but numerically they're far (23 vs 0). Cyclical encoding fixes this.</p>
        <pre style="font-size: 7.5pt;">
# Hour (24-hour cycle)
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)

# Day of week (7-day cycle)
df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)

# Month (12-month cycle)
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        </pre>
    </div>
    
    <h2>Domain-Specific Features</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th style="width: 20%;">Domain</th>
            <th style="width: 40%;">Feature Ideas</th>
            <th style="width: 40%;">Example Code</th>
        </tr>
        <tr>
            <td><strong>E-Commerce</strong></td>
            <td>‚Ä¢ Avg order value<br>‚Ä¢ Purchase frequency<br>‚Ä¢ Days since last purchase<br>‚Ä¢ Discount %</td>
            <td><code style="font-size: 7.5pt;">df['avg_order'] = df['total_spent'] / df['num_orders']<br>df['discount_pct'] = (df['orig_price'] - df['sale_price']) / df['orig_price']</code></td>
        </tr>
        <tr>
            <td><strong>Finance</strong></td>
            <td>‚Ä¢ Debt-to-income ratio<br>‚Ä¢ Credit utilization<br>‚Ä¢ Payment history<br>‚Ä¢ Income volatility</td>
            <td><code style="font-size: 7.5pt;">df['debt_to_income'] = df['debt'] / df['income']<br>df['credit_util'] = df['balance'] / df['limit']</code></td>
        </tr>
        <tr>
            <td><strong>Healthcare</strong></td>
            <td>‚Ä¢ BMI<br>‚Ä¢ Age groups<br>‚Ä¢ Risk scores<br>‚Ä¢ Interaction effects</td>
            <td><code style="font-size: 7.5pt;">df['bmi'] = df['weight'] / (df['height']**2)<br>df['high_risk'] = (df['age'] > 60) & (df['bp'] > 140)</code></td>
        </tr>
        <tr>
            <td><strong>Real Estate</strong></td>
            <td>‚Ä¢ Price per sqft<br>‚Ä¢ Sqft per bedroom<br>‚Ä¢ Distance to amenities<br>‚Ä¢ Age of property</td>
            <td><code style="font-size: 7.5pt;">df['price_per_sqft'] = df['price'] / df['sqft']<br>df['property_age'] = 2025 - df['year_built']</code></td>
        </tr>
    </table>
    
    <h2>Interaction Features</h2>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è When to Create Interactions</h4>
        <p style="font-size: 9pt;"><strong>Use for Linear Models:</strong> They can't find interactions automatically<br>
        <strong>Skip for Tree Models:</strong> Trees find interactions naturally<br>
        <strong>Use Domain Knowledge:</strong> Don't blindly create all combinations</p>
    </div>
    
    <pre style="font-size: 8pt; background: var(--light-gray); color: var(--gray); padding: 8pt; border-radius: 4pt;">
# Manual meaningful interactions
df['income_per_dependent'] = df['income'] / (df['dependents'] + 1)
df['house_value_to_income'] = df['house_value'] / df['income']

# Categorical √ó Numerical (creates different slopes)
df['premium_spending'] = df['is_premium'] * df['total_spent']

# Polynomial features (for linear models)
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X[['feature1', 'feature2']])
    </pre>
    
    <div class="box danger">
        <h4>üö´ Common Feature Engineering Mistakes</h4>
        <ul style="font-size: 9pt;">
            <li><strong>Creating features from test data:</strong> Always split BEFORE engineering to avoid leakage</li>
            <li><strong>Too many polynomial features:</strong> 10 features, degree 3 ‚Üí 220 features (overfitting!)</li>
            <li><strong>Not removing useless features:</strong> Check feature importance, remove low-value features</li>
            <li><strong>Forgetting to save transformations:</strong> Must apply same transformations at inference time</li>
        </ul>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ Feature Engineering Checklist</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>Before Engineering:</strong><br>
                ‚òë EDA to understand data<br>
                ‚òë Split data first<br>
                ‚òë Identify feature types<br>
                ‚òë Check missing values</p>
            </div>
            <div>
                <p><strong>After Engineering:</strong><br>
                ‚òë Validate improves model<br>
                ‚òë Check for leakage<br>
                ‚òë Remove low-importance features<br>
                ‚òë Document transformations</p>
            </div>
        </div>
    </div>
    
    <div class="page-number">Page 8</div>
</div>

<!-- PAGE 9: STATISTICAL TESTS -->
<div class="page">
    <h1>üìä Statistical Tests Selection</h1>
    
    <h2>Test Selection Flowchart</h2>
    
    <div class="decision-tree">
What are you comparing?

TWO CATEGORICAL VARIABLES
‚îî‚îÄ Use: <strong>Chi-Square Test</strong>
   Example: Gender vs Product Preference
   Check: Expected count ‚â• 5 in 80% of cells
   
ONE CATEGORICAL + ONE NUMERICAL
‚îú‚îÄ 2 Groups?
‚îÇ  ‚îú‚îÄ Independent samples?
‚îÇ  ‚îÇ  ‚îú‚îÄ Normal? ‚Üí <strong>Independent t-test</strong>
‚îÇ  ‚îÇ  ‚îî‚îÄ NOT normal? ‚Üí <strong>Mann-Whitney U</strong>
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ Paired (before/after)?
‚îÇ    ‚îú‚îÄ Normal? ‚Üí <strong>Paired t-test</strong>
‚îÇ    ‚îî‚îÄ NOT normal? ‚Üí <strong>Wilcoxon</strong>
‚îÇ
‚îî‚îÄ 3+ Groups?
   ‚îú‚îÄ Normal? ‚Üí <strong>One-Way ANOVA</strong>
   ‚îî‚îÄ NOT normal? ‚Üí <strong>Kruskal-Wallis</strong>
   
TWO NUMERICAL VARIABLES
‚îú‚îÄ Linear relationship? ‚Üí <strong>Pearson Correlation</strong>
‚îî‚îÄ Monotonic? ‚Üí <strong>Spearman Correlation</strong>
    </div>
    
    <h2>Test Comparison Matrix</h2>
    
    <table style="font-size: 8pt;">
        <tr>
            <th>Test</th>
            <th># Groups</th>
            <th>Data Type</th>
            <th>Paired?</th>
            <th>Parametric?</th>
            <th>When to Use</th>
        </tr>
        <tr>
            <td><strong>Chi-Square</strong></td>
            <td>2+</td>
            <td>Categorical</td>
            <td>No</td>
            <td>Non-param</td>
            <td>Test independence of 2 categorical variables</td>
        </tr>
        <tr>
            <td><strong>t-test</strong></td>
            <td>2</td>
            <td>Continuous</td>
            <td>No</td>
            <td>Yes</td>
            <td>Compare means, normal data</td>
        </tr>
        <tr>
            <td><strong>Paired t-test</strong></td>
            <td>2</td>
            <td>Continuous</td>
            <td>Yes</td>
            <td>Yes</td>
            <td>Before/after, normal data</td>
        </tr>
        <tr>
            <td><strong>Mann-Whitney U</strong></td>
            <td>2</td>
            <td>Ordinal/Continuous</td>
            <td>No</td>
            <td>Non-param</td>
            <td>Compare 2 groups, non-normal</td>
        </tr>
        <tr>
            <td><strong>Wilcoxon</strong></td>
            <td>2</td>
            <td>Ordinal/Continuous</td>
            <td>Yes</td>
            <td>Non-param</td>
            <td>Paired data, non-normal</td>
        </tr>
        <tr>
            <td><strong>ANOVA</strong></td>
            <td>3+</td>
            <td>Continuous</td>
            <td>No</td>
            <td>Yes</td>
            <td>Compare 3+ groups, normal</td>
        </tr>
        <tr>
            <td><strong>Kruskal-Wallis</strong></td>
            <td>3+</td>
            <td>Ordinal/Continuous</td>
            <td>No</td>
            <td>Non-param</td>
            <td>Compare 3+ groups, non-normal</td>
        </tr>
    </table>
    
    <h2>Chi-Square vs T-test vs ANOVA</h2>
    
    <div class="grid-3" style="font-size: 8.5pt;">
        <div class="card" style="border-left: 4px solid var(--red);">
            <h4 style="color: var(--red);">Chi-Square</h4>
            <p><strong>Use when:</strong><br>
            ‚Ä¢ Both variables categorical<br>
            ‚Ä¢ Testing independence<br>
            ‚Ä¢ Have contingency table</p>
            <p><strong>Example:</strong><br>
            Smoking (Yes/No) vs<br>Lung Cancer (Yes/No)</p>
            <p><strong>Don't use if:</strong><br>
            ‚Ä¢ Expected count &lt; 5 in many cells<br>
            ‚Üí Use Fisher's Exact</p>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--primary-blue);">
            <h4 style="color: var(--primary-blue);">T-test</h4>
            <p><strong>Use when:</strong><br>
            ‚Ä¢ Comparing 2 groups only<br>
            ‚Ä¢ Continuous outcome<br>
            ‚Ä¢ Normal distribution</p>
            <p><strong>Example:</strong><br>
            Male vs Female salaries</p>
            <p><strong>Don't use if:</strong><br>
            ‚Ä¢ Non-normal data<br>
            ‚Üí Use Mann-Whitney U<br>
            ‚Ä¢ 3+ groups ‚Üí Use ANOVA</p>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--orange);">
            <h4 style="color: var(--orange);">ANOVA</h4>
            <p><strong>Use when:</strong><br>
            ‚Ä¢ Comparing 3+ groups<br>
            ‚Ä¢ Continuous outcome<br>
            ‚Ä¢ Normal distribution</p>
            <p><strong>Example:</strong><br>
            Drug A vs B vs C vs Placebo</p>
            <p><strong>Follow-up:</strong><br>
            If significant ‚Üí Run Tukey HSD<br>
            to find which groups differ</p>
        </div>
    </div>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è Common Statistical Test Mistakes</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>‚ùå Not checking assumptions</strong><br>
                Always check normality (Shapiro-Wilk), equal variance (Levene's test)</p>
                <p><strong>‚ùå Using wrong test for data type</strong><br>
                Can't use t-test on categorical data!</p>
            </div>
            <div>
                <p><strong>‚ùå Multiple comparisons without correction</strong><br>
                Running 20 tests? Use Bonferroni correction</p>
                <p><strong>‚ùå Confusing significance with effect size</strong><br>
                p&lt;0.05 doesn't mean large effect. Report Cohen's d, Cram√©r's V</p>
            </div>
        </div>
    </div>
    
    <div class="box info">
        <h4>üìå Quick Reference: P-value Interpretation</h4>
        <ul style="font-size: 9pt;">
            <li><strong>p &lt; 0.01:</strong> Very strong evidence against null hypothesis</li>
            <li><strong>p &lt; 0.05:</strong> Strong evidence (standard threshold)</li>
            <li><strong>p &lt; 0.10:</strong> Weak evidence (sometimes used in exploratory analysis)</li>
            <li><strong>p ‚â• 0.05:</strong> Fail to reject null (no evidence of difference)</li>
        </ul>
        <p style="font-size: 9pt; margin-top: 8pt;"><strong>Remember:</strong> p-value is NOT the probability that null is true. It's the probability of seeing your data (or more extreme) if null were true.</p>
    </div>
    
    <div class="page-number">Page 9</div>
</div>

<!-- PAGE 10: A/B TESTING -->
<div class="page">
    <h1>üß™ A/B Testing Best Practices</h1>
    
    <h2>Sample Size Calculation (BEFORE Running Test!)</h2>
    
    <pre style="font-size: 7.5pt;">
import scipy.stats as stats
import numpy as np

def calculate_sample_size(baseline_rate, mde, alpha=0.05, power=0.80):
    """
    baseline_rate: Current conversion rate (e.g., 0.05 for 5%)
    mde: Minimum detectable effect (e.g., 0.10 for 10% relative lift)
    alpha: Significance level (typically 0.05)
    power: Statistical power (typically 0.80)
    """
    p1 = baseline_rate
    p2 = baseline_rate * (1 + mde)
    p_pooled = (p1 + p2) / 2
    
    z_alpha = stats.norm.ppf(1 - alpha / 2)
    z_beta = stats.norm.ppf(power)
    
    n = ((z_alpha * np.sqrt(2*p_pooled*(1-p_pooled)) +
          z_beta * np.sqrt(p1*(1-p1) + p2*(1-p2)))**2) / ((p2-p1)**2)
    
    return int(np.ceil(n))

# Example
sample_size = calculate_sample_size(baseline_rate=0.05, mde=0.10)
print(f"Need {sample_size:,} per variant (total: {sample_size*2:,})")
    </pre>
    
    <h2>Test Duration Guidelines</h2>
    
    <table style="font-size: 9pt;">
        <tr>
            <th>Factor</th>
            <th>Minimum</th>
            <th>Recommended</th>
            <th>Why</th>
        </tr>
        <tr>
            <td><strong>Duration</strong></td>
            <td>1 week</td>
            <td>2-4 weeks</td>
            <td>Capture weekly patterns, avoid day-of-week bias</td>
        </tr>
        <tr>
            <td><strong>Sample Size</strong></td>
            <td>Calculate first!</td>
            <td>Use power analysis</td>
            <td>Ensure enough power to detect effect</td>
        </tr>
        <tr>
            <td><strong>Traffic Split</strong></td>
            <td>50/50</td>
            <td>50/50 or 90/10</td>
            <td>50/50 for equal power, 90/10 for low risk</td>
        </tr>
        <tr>
            <td><strong>Business Cycles</strong></td>
            <td>1 full cycle</td>
            <td>2+ cycles</td>
            <td>Account for weekend/weekday differences</td>
        </tr>
    </table>
    
    <h2>Analyzing Results</h2>
    
    <pre style="font-size: 7.5pt;">
def analyze_ab_test(conv_a, visit_a, conv_b, visit_b):
    """Analyze A/B test with statistical significance"""
    rate_a = conv_a / visit_a
    rate_b = conv_b / visit_b
    
    # Pooled proportion
    p_pooled = (conv_a + conv_b) / (visit_a + visit_b)
    
    # Standard error
    se = np.sqrt(p_pooled * (1-p_pooled) * (1/visit_a + 1/visit_b))
    
    # Z-score and p-value
    z_score = (rate_b - rate_a) / se
    p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
    
    # Relative lift
    relative_lift = ((rate_b - rate_a) / rate_a) * 100
    
    # 95% Confidence interval
    margin = 1.96 * se
    ci = (rate_b - rate_a - margin, rate_b - rate_a + margin)
    
    return {
        'rate_a': rate_a, 'rate_b': rate_b,
        'lift': relative_lift, 'p_value': p_value,
        'ci': ci, 'significant': p_value < 0.05
    }
    </pre>
    
    <div class="box danger">
        <h4>üö´ A/B Testing Pitfalls</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>‚ùå Peeking (Multiple Testing)</strong><br>
                Checking results daily and stopping when p&lt;0.05 inflates false positive rate!</p>
                <p><strong>Solution:</strong> Wait until planned sample size reached, or use sequential testing</p>
            </div>
            <div>
                <p><strong>‚ùå Stopping Too Early</strong><br>
                3 days of data may show noise, not real effect</p>
                <p><strong>Solution:</strong> Run for at least 2 weeks, reach sample size</p>
            </div>
            <div>
                <p><strong>‚ùå Ignoring Novelty Effect</strong><br>
                Users click new design out of curiosity</p>
                <p><strong>Solution:</strong> Run longer (2-4 weeks), segment new vs returning users</p>
            </div>
            <div>
                <p><strong>‚ùå Testing Too Many Variants</strong><br>
                Testing A vs B vs C vs D vs E needs 5x traffic</p>
                <p><strong>Solution:</strong> Test 2 variants at a time, or use proper multivariate framework</p>
            </div>
        </div>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ A/B Testing Checklist</h4>
        <div style="columns: 2; column-gap: 15pt; font-size: 9pt;">
            <p><strong>Before Test:</strong><br>
            ‚òë Calculate required sample size<br>
            ‚òë Define success metric<br>
            ‚òë Set significance level (Œ±=0.05)<br>
            ‚òë Plan duration (min 2 weeks)<br>
            ‚òë Randomize assignment</p>
            
            <p><strong>During Test:</strong><br>
            ‚òë Don't peek early!<br>
            ‚òë Monitor for bugs/issues<br>
            ‚òë Ensure consistent experience<br>
            ‚òë Track multiple metrics</p>
            
            <p><strong>After Test:</strong><br>
            ‚òë Check statistical significance<br>
            ‚òë Report effect size & CI<br>
            ‚òë Consider practical significance<br>
            ‚òë Segment analysis (if relevant)<br>
            ‚òë Document learnings</p>
        </div>
    </div>
    
    <div class="box info">
        <h4>üìå Interpreting Results</h4>
        <table style="font-size: 9pt; width: 100%;">
            <tr>
                <th>Scenario</th>
                <th>Decision</th>
            </tr>
            <tr>
                <td>p &lt; 0.05, lift &gt; 5%</td>
                <td><strong>Ship it!</strong> Statistically and practically significant</td>
            </tr>
            <tr>
                <td>p &lt; 0.05, lift &lt; 2%</td>
                <td><strong>Consider:</strong> Significant but small effect. Worth the effort?</td>
            </tr>
            <tr>
                <td>p ‚â• 0.05, trending positive</td>
                <td><strong>Run longer</strong> or accept no difference</td>
            </tr>
            <tr>
                <td>p ‚â• 0.05, CI includes 0</td>
                <td><strong>No evidence</strong> of difference. Don't ship.</td>
            </tr>
        </tr>
        </table>
    </div>
    
    <div class="page-number">Page 10</div>
</div>

<!-- PAGE 11: DEEP LEARNING FUNDAMENTALS -->
<div class="page">
    <h1>üß† Deep Learning: When to Use</h1>
    
    <h2>Should You Use Deep Learning?</h2>
    
    <div class="decision-tree">
Data Type?
‚îú‚îÄ TABULAR/STRUCTURED
‚îÇ  ‚îî‚îÄ ‚ùå Use XGBoost/Random Forest instead
‚îÇ    Exception: 1M+ rows with complex interactions
‚îÇ
‚îú‚îÄ IMAGES ‚Üí ‚úÖ Use CNNs
‚îú‚îÄ TEXT ‚Üí ‚úÖ Use Transformers
‚îú‚îÄ AUDIO/VIDEO ‚Üí ‚úÖ Use specialized architectures
‚îî‚îÄ TIME SERIES ‚Üí ‚ö†Ô∏è Try traditional methods first (ARIMA, Prophet)

Sample Size?
‚îú‚îÄ &lt; 10K ‚Üí ‚ùå Too small, use Traditional ML
‚îú‚îÄ 10K-100K ‚Üí ‚ö†Ô∏è Transfer learning OR Traditional ML
‚îî‚îÄ &gt; 100K ‚Üí ‚úÖ Deep Learning viable

Resources?
‚îú‚îÄ No GPU ‚Üí ‚ùå Use Traditional ML (10-100x faster)
‚îú‚îÄ Limited time ‚Üí ‚ùå XGBoost trains faster, less tuning
‚îî‚îÄ Have GPU + time ‚Üí ‚úÖ Deep Learning possible

Need Interpretability?
‚îú‚îÄ YES ‚Üí ‚ùå Use Linear/Tree models
‚îî‚îÄ NO ‚Üí ‚úÖ Deep Learning OK
    </div>
    
    <h2>When Deep Learning Wins</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card" style="border-left: 4px solid var(--green);">
            <h4 style="color: var(--green);">‚úÖ Use Deep Learning</h4>
            <ul style="font-size: 8.5pt; line-height: 1.5;">
                <li><strong>Unstructured data:</strong> Images, audio, video</li>
                <li><strong>NLP tasks:</strong> Translation, generation, Q&A</li>
                <li><strong>Large datasets:</strong> 100K+ samples</li>
                <li><strong>Complex patterns:</strong> Non-linear relationships</li>
                <li><strong>Automatic features:</strong> Don't want manual engineering</li>
                <li><strong>Have GPU:</strong> 10-100x faster training</li>
            </ul>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--red);">
            <h4 style="color: var(--red);">‚ùå Don't Use Deep Learning</h4>
            <ul style="font-size: 8.5pt; line-height: 1.5;">
                <li><strong>Tabular data:</strong> XGBoost wins 90% of time</li>
                <li><strong>Small datasets:</strong> &lt;10K samples</li>
                <li><strong>Need interpretability:</strong> Healthcare, finance, legal</li>
                <li><strong>No GPU:</strong> Training too slow</li>
                <li><strong>Quick prototyping:</strong> XGBoost faster to iterate</li>
                <li><strong>Production constraints:</strong> Models are large, slow inference</li>
            </ul>
        </div>
    </div>
    
    <h2>Architecture Selection</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th style="width: 20%;">Architecture</th>
            <th style="width: 20%;">Data Type</th>
            <th style="width: 30%;">Use Cases</th>
            <th style="width: 30%;">Key Strength</th>
        </tr>
        <tr>
            <td><strong>MLP</strong><br>(Fully Connected)</td>
            <td>Tabular</td>
            <td>Classification, Regression on structured data</td>
            <td>Simple baseline, but XGBoost usually better</td>
        </tr>
        <tr>
            <td><strong>CNN</strong><br>(Convolutional)</td>
            <td>Images, Spatial</td>
            <td>Image classification, Object detection</td>
            <td>Captures spatial relationships</td>
        </tr>
        <tr>
            <td><strong>RNN/LSTM</strong></td>
            <td>Sequences</td>
            <td>Time series, Text (older approach)</td>
            <td>Memory of past inputs</td>
        </tr>
        <tr>
            <td><strong>Transformer</strong></td>
            <td>Text, Sequences</td>
            <td>NLP (BERT, GPT), Translation</td>
            <td>Long-range dependencies, parallelizable</td>
        </tr>
        <tr>
            <td><strong>Autoencoder</strong></td>
            <td>Any</td>
            <td>Dimensionality reduction, Denoising</td>
            <td>Unsupervised learning</td>
        </tr>
        <tr>
            <td><strong>GAN</strong></td>
            <td>Images</td>
            <td>Image generation, Style transfer</td>
            <td>Generative modeling</td>
        </tr>
    </table>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è Reality Check: Tabular Data</h4>
        <p style="font-size: 9pt;"><strong>90% of Kaggle tabular competitions are won by XGBoost/LightGBM, not neural networks.</strong></p>
        <p style="font-size: 9pt;">Why?<br>
        ‚Ä¢ Trees handle mixed data types better<br>
        ‚Ä¢ Trees handle missing values natively<br>
        ‚Ä¢ Trees need less preprocessing<br>
        ‚Ä¢ Trees train faster<br>
        ‚Ä¢ Trees are more interpretable</p>
        <p style="font-size: 9pt; margin-top: 6pt;"><strong>Exception:</strong> Very large datasets (1M+ rows) with complex interactions, neural nets can work.</p>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ Deep Learning Decision Framework</h4>
        <ol style="font-size: 9pt;">
            <li><strong>Start with baseline:</strong> Always try simple model first (Logistic Reg for classification)</li>
            <li><strong>Try traditional ML:</strong> XGBoost for tabular, classical methods for time series</li>
            <li><strong>Only then try DL:</strong> If traditional ML fails AND you have enough data/compute</li>
            <li><strong>Use transfer learning:</strong> For images/text with limited data, fine-tune pretrained models</li>
        </ol>
    </div>
    
    <div class="page-number">Page 11</div>
</div>

<!-- PAGE 12: NEURAL NETWORK TRAINING -->
<div class="page">
    <h1>‚öôÔ∏è Neural Network Training Guide</h1>
    
    <h2>Key Hyperparameters</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th style="width: 20%;">Hyperparameter</th>
            <th style="width: 25%;">Typical Values</th>
            <th style="width: 25%;">Impact</th>
            <th style="width: 30%;">How to Choose</th>
        </tr>
        <tr>
            <td><strong>Learning Rate</strong></td>
            <td>0.001 (Adam)<br>0.01 (SGD)</td>
            <td><span class="highlight">Most important!</span><br>Too high: diverges<br>Too low: slow</td>
            <td>Use learning rate finder, start with 0.001</td>
        </tr>
        <tr>
            <td><strong>Batch Size</strong></td>
            <td>32, 64, 128</td>
            <td>Small: better generalization<br>Large: faster, stable</td>
            <td>Start with 32-64, use powers of 2</td>
        </tr>
        <tr>
            <td><strong>Optimizer</strong></td>
            <td>Adam (default)<br>SGD+momentum<br>AdamW (NLP)</td>
            <td>Affects convergence speed</td>
            <td>Adam for most tasks, AdamW for Transformers</td>
        </tr>
        <tr>
            <td><strong>Dropout</strong></td>
            <td>0.2-0.5</td>
            <td>Regularization, prevents overfitting</td>
            <td>Start with 0.5 for FC layers, 0.2-0.3 for CNN</td>
        </tr>
        <tr>
            <td><strong>Weight Decay</strong></td>
            <td>1e-5 to 1e-4</td>
            <td>L2 regularization</td>
            <td>Start with 1e-5, increase if overfitting</td>
        </tr>
    </table>
    
    <h2>Training Best Practices</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">Data Preparation</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>Normalize inputs:</strong> Images to [0,1] or standardize</li>
                <li><strong>Use data augmentation:</strong> For images (flip, rotate, crop)</li>
                <li><strong>Balance classes:</strong> Oversample minority or use class weights</li>
                <li><strong>Split properly:</strong> Train/val/test before any processing</li>
            </ul>
        </div>
        
        <div class="card">
            <h4 style="color: var(--teal);">Regularization</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>Dropout:</strong> Add between layers (0.5 for dense)</li>
                <li><strong>Batch Normalization:</strong> Stabilizes training</li>
                <li><strong>Early Stopping:</strong> Stop when val loss stops improving</li>
                <li><strong>Weight Decay:</strong> L2 regularization (1e-5)</li>
            </ul>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">Monitoring</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>Track both:</strong> Training AND validation loss</li>
                <li><strong>Plot curves:</strong> Visualize loss over epochs</li>
                <li><strong>Watch for overfitting:</strong> Val loss increasing while train decreasing</li>
                <li><strong>Use TensorBoard:</strong> For experiment tracking</li>
            </ul>
        </div>
        
        <div class="card">
            <h4 style="color: var(--green);">Optimization</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>Use GPU:</strong> 10-100x faster than CPU</li>
                <li><strong>Learning rate scheduler:</strong> Reduce on plateau</li>
                <li><strong>Gradient clipping:</strong> Prevent exploding gradients</li>
                <li><strong>Mixed precision:</strong> Faster training on modern GPUs</li>
            </ul>
        </div>
    </div>
    
    <div class="box danger">
        <h4>üö´ Common Neural Network Mistakes</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>‚ùå Not normalizing inputs</strong></p>
                <pre style="font-size: 7pt;">
# WRONG - raw pixel values
X = images  # [0, 255]

# CORRECT
X = images / 255.0  # [0, 1]
                </pre>
            </div>
            <div>
                <p><strong>‚ùå Forgetting model.eval()</strong></p>
                <pre style="font-size: 7pt;">
# WRONG - dropout still active!
predictions = model(test_data)

# CORRECT
model.eval()
with torch.no_grad():
    predictions = model(test_data)
                </pre>
            </div>
            <div>
                <p><strong>‚ùå Only checking train loss</strong></p>
                <pre style="font-size: 7pt;">
# WRONG - might be overfitting!
print(f"Train loss: {train_loss}")

# CORRECT - monitor both
print(f"Train: {train_loss}")
print(f"Val: {val_loss}")
if val_loss > train_loss * 1.5:
    print("Overfitting!")
                </pre>
            </div>
            <div>
                <p><strong>‚ùå Not using GPU when available</strong></p>
                <pre style="font-size: 7pt;">
# WRONG - slow CPU training
model = Model()

# CORRECT
device = torch.device("cuda" if 
    torch.cuda.is_available() else "cpu")
model = Model().to(device)
data = data.to(device)
                </pre>
            </div>
        </div>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ Training Workflow</h4>
        <ol style="font-size: 9pt;">
            <li><strong>Prepare data:</strong> Normalize, augment, split</li>
            <li><strong>Start simple:</strong> Small model first, then scale up</li>
            <li><strong>Baseline:</strong> Train for few epochs with defaults</li>
            <li><strong>Tune LR:</strong> Use learning rate finder</li>
            <li><strong>Add regularization:</strong> Dropout, weight decay if overfitting</li>
            <li><strong>Early stopping:</strong> Save best model on val loss</li>
            <li><strong>Evaluate:</strong> Test on holdout set</li>
        </ol>
    </div>
    
    <div class="page-number">Page 12</div>
</div>

<!-- PAGE 13: NLP & TRANSFORMERS -->
<div class="page">
    <h1>üìù NLP & Transformers Quick Reference</h1>
    
    <h2>NLP Task Selection</h2>
    
    <table style="font-size: 8pt;">
        <tr>
            <th style="width: 20%;">Task</th>
            <th style="width: 20%;">Sample Size</th>
            <th style="width: 30%;">Best Model</th>
            <th style="width: 30%;">Notes</th>
        </tr>
        <tr>
            <td><strong>Text Classification</strong><br>(Sentiment, Topic)</td>
            <td>&lt;1K<br>1K-10K<br>10K+</td>
            <td>TF-IDF + LogReg<br>Fine-tune BERT/RoBERTa<br>Fine-tune RoBERTa-large</td>
            <td>Simple baseline works well for small data</td>
        </tr>
        <tr>
            <td><strong>Text Generation</strong><br>(Stories, Articles)</td>
            <td>Any</td>
            <td>GPT-4 (best quality)<br>GPT-3.5 (cost-effective)<br>Llama 3 / Mistral (open)</td>
            <td>Use API or self-host open models</td>
        </tr>
        <tr>
            <td><strong>Question Answering</strong></td>
            <td>1K+</td>
            <td>Fine-tune BERT<br>RoBERTa<br>DeBERTa</td>
            <td>Extractive: answer in passage<br>Generative: create answer</td>
        </tr>
        <tr>
            <td><strong>Named Entity Recognition</strong></td>
            <td>1K+</td>
            <td>Fine-tune BERT<br>RoBERTa<br>SpaCy (fast baseline)</td>
            <td>Tag entities: person, location, org</td>
        </tr>
        <tr>
            <td><strong>Summarization</strong></td>
            <td>Any</td>
            <td>BART<br>T5<br>GPT-4</td>
            <td>T5 good for open-source, GPT-4 for quality</td>
        </tr>
        <tr>
            <td><strong>Translation</strong></td>
            <td>-</td>
            <td>MarianMT (specific pairs)<br>T5<br>GPT-4</td>
            <td>MarianMT fast, GPT-4 multi-language</td>
        </tr>
        <tr>
            <td><strong>Semantic Similarity</strong></td>
            <td>-</td>
            <td>Sentence-BERT (SBERT)<br>Universal Sentence Encoder</td>
            <td>Embed sentences, compute cosine similarity</td>
        </tr>
    </table>
    
    <h2>Popular Transformers (2025)</h2>
    
    <div class="grid-3" style="font-size: 8.5pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">BERT Family</h4>
            <p style="font-size: 8pt;"><strong>BERT:</strong> Encoder-only, bidirectional<br>
            <strong>Use for:</strong> Classification, NER, Q&A<br>
            <strong>Size:</strong> 110M params (base)<br>
            <strong>Fine-tune on:</strong> 1K+ samples</p>
            <p style="font-size: 8pt; margin-top: 6pt;"><strong>Variants:</strong><br>
            ‚Ä¢ RoBERTa (optimized BERT)<br>
            ‚Ä¢ DistilBERT (faster, smaller)<br>
            ‚Ä¢ DeBERTa (improved)</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--green);">GPT Family</h4>
            <p style="font-size: 8pt;"><strong>GPT:</strong> Decoder-only, autoregressive<br>
            <strong>Use for:</strong> Generation, completion<br>
            <strong>Size:</strong> 1.5B - 175B params<br>
            <strong>Access:</strong> OpenAI API</p>
            <p style="font-size: 8pt; margin-top: 6pt;"><strong>Versions:</strong><br>
            ‚Ä¢ GPT-3.5: Cost-effective<br>
            ‚Ä¢ GPT-4: Highest quality<br>
            ‚Ä¢ Llama 3: Open-source alt</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">T5 / BART</h4>
            <p style="font-size: 8pt;"><strong>T5:</strong> Encoder-decoder, multi-task<br>
            <strong>Use for:</strong> Translation, summarization<br>
            <strong>Size:</strong> 60M - 11B params<br>
            <strong>Framework:</strong> "text-to-text"</p>
            <p style="font-size: 8pt; margin-top: 6pt;"><strong>BART:</strong><br>
            Similar to T5, good for summarization</p>
        </div>
    </div>
    
    <div class="box info">
        <h4>üéØ Quick Decision: Which Transformer?</h4>
        <div class="decision-tree" style="font-size: 9pt;">
What's your task?

CLASSIFICATION / NER / Q&A ‚Üí <strong>BERT or RoBERTa</strong>
‚îú‚îÄ Need speed? ‚Üí DistilBERT
‚îî‚îÄ Need quality? ‚Üí DeBERTa

GENERATION / COMPLETION ‚Üí <strong>GPT Family</strong>
‚îú‚îÄ Budget unlimited? ‚Üí GPT-4
‚îú‚îÄ Cost-conscious? ‚Üí GPT-3.5
‚îî‚îÄ Self-host? ‚Üí Llama 3 / Mistral

TRANSLATION / SUMMARIZATION ‚Üí <strong>T5 or BART</strong>
‚îî‚îÄ Or use GPT-4 if budget allows

EMBEDDINGS / SIMILARITY ‚Üí <strong>Sentence-BERT</strong>
        </div>
    </div>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è Transformer Training Tips</h4>
        <ul style="font-size: 9pt;">
            <li><strong>Learning Rate:</strong> Typically 1e-5 to 5e-5 (much lower than CNNs!)</li>
            <li><strong>Batch Size:</strong> 8-32 (limited by GPU memory)</li>
            <li><strong>Epochs:</strong> 2-5 epochs usually sufficient (overfits quickly)</li>
            <li><strong>Optimizer:</strong> AdamW (Adam with weight decay)</li>
            <li><strong>Warmup:</strong> Use learning rate warmup (500-1000 steps)</li>
            <li><strong>Max Length:</strong> Truncate to 512 tokens (BERT limit)</li>
        </ul>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ NLP Best Practices</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>Data Preparation:</strong><br>
                ‚Ä¢ Clean text (remove HTML, special chars)<br>
                ‚Ä¢ Lowercase (unless case matters)<br>
                ‚Ä¢ Handle emojis appropriately<br>
                ‚Ä¢ Balance classes if needed</p>
            </div>
            <div>
                <p><strong>Model Selection:</strong><br>
                ‚Ä¢ Start simple (TF-IDF + LogReg)<br>
                ‚Ä¢ Try pretrained models<br>
                ‚Ä¢ Fine-tune if have 1K+ samples<br>
                ‚Ä¢ Use few-shot GPT if &lt;1K samples</p>
            </div>
        </div>
    </div>
    
    <div class="page-number">Page 13</div>
</div>

<!-- PAGE 14: EVALUATION METRICS - CLASSIFICATION -->
<div class="page">
    <h1>üìä Evaluation Metrics: Classification</h1>
    
    <h2>Confusion Matrix</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div>
            <table style="font-size: 9pt;">
                <tr>
                    <th colspan="2" rowspan="2"></th>
                    <th colspan="2" style="text-align: center;">Predicted</th>
                </tr>
                <tr>
                    <th>Positive</th>
                    <th>Negative</th>
                </tr>
                <tr>
                    <th rowspan="2" style="writing-mode: vertical-lr; text-align: center;">Actual</th>
                    <th>Positive</th>
                    <td style="background: #d5f4e6; font-weight: 600;">TP<br>True Positive</td>
                    <td style="background: #ffe6e6;">FN<br>False Negative</td>
                </tr>
                <tr>
                    <th>Negative</th>
                    <td style="background: #ffe6e6;">FP<br>False Positive</td>
                    <td style="background: #d5f4e6; font-weight: 600;">TN<br>True Negative</td>
                </tr>
            </table>
        </div>
        
        <div>
            <h4>Derived Metrics:</h4>
            <p style="font-size: 9pt;"><strong>Accuracy</strong> = (TP + TN) / Total<br>
            <strong>Precision</strong> = TP / (TP + FP)<br>
            <strong>Recall</strong> = TP / (TP + FN)<br>
            <strong>F1-Score</strong> = 2 √ó (P √ó R) / (P + R)</p>
        </div>
    </div>
    
    <h2>When to Use Each Metric</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th style="width: 20%;">Metric</th>
            <th style="width: 25%;">Formula</th>
            <th style="width: 30%;">When to Use</th>
            <th style="width: 25%;">Example</th>
        </tr>
        <tr>
            <td><strong>Accuracy</strong></td>
            <td>(TP+TN) / Total</td>
            <td><strong>Balanced classes only</strong><br>Equal importance for both classes</td>
            <td>Email: spam vs not spam<br>(50/50 split)</td>
        </tr>
        <tr>
            <td><strong>Precision</strong></td>
            <td>TP / (TP+FP)</td>
            <td><strong>False Positives costly</strong><br>Want to be sure when predicting positive</td>
            <td>Spam filter: Don't mark real email as spam</td>
        </tr>
        <tr>
            <td><strong>Recall</strong></td>
            <td>TP / (TP+FN)</td>
            <td><strong>False Negatives costly</strong><br>Must catch all positives</td>
            <td>Cancer detection: Don't miss any cases</td>
        </tr>
        <tr>
            <td><strong>F1-Score</strong></td>
            <td>2√ó(P√óR)/(P+R)</td>
            <td><strong>Imbalanced classes</strong><br>Balance precision & recall</td>
            <td>Fraud detection<br>(1% fraud rate)</td>
        </tr>
        <tr>
            <td><strong>ROC-AUC</strong></td>
            <td>Area under ROC curve</td>
            <td><strong>Binary classification</strong><br>Threshold-agnostic<br>Balanced classes</td>
            <td>Credit scoring:<br>Good/Bad credit</td>
        </tr>
        <tr>
            <td><strong>PR-AUC</strong></td>
            <td>Area under P-R curve</td>
            <td><strong>Imbalanced classes</strong><br>Better than ROC-AUC</td>
            <td>Rare disease detection<br>(0.1% prevalence)</td>
        </tr>
        <tr>
            <td><strong>Log Loss</strong></td>
            <td>-Œ£ y log(p)</td>
            <td><strong>Probabilistic predictions</strong><br>Penalizes wrong confidence</td>
            <td>When need calibrated probabilities</td>
        </tr>
    </table>
    
    <div class="box danger">
        <h4>üö´ Common Metric Mistakes</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>‚ùå Using Accuracy for Imbalanced Data</strong></p>
                <p style="font-size: 8.5pt;">Example: 99% negative class<br>
                Model predicting "all negative" gets 99% accuracy!<br>
                <strong>‚úÖ Use F1-Score or PR-AUC instead</strong></p>
            </div>
            <div>
                <p><strong>‚ùå Ignoring Business Context</strong></p>
                <p style="font-size: 8.5pt;">High precision means few false alarms, but might miss true cases.<br>
                High recall means catch everything, but many false alarms.<br>
                <strong>‚úÖ Choose based on business cost</strong></p>
            </div>
        </div>
    </div>
    
    <h2>Precision vs Recall Tradeoff</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card" style="border-left: 4px solid var(--primary-blue);">
            <h4 style="color: var(--primary-blue);">High Precision Scenarios</h4>
            <p style="font-size: 8.5pt;"><strong>When False Positives are costly:</strong></p>
            <ul style="font-size: 8pt;">
                <li>Spam filter (don't mark real email as spam)</li>
                <li>Product recommendations (don't recommend bad products)</li>
                <li>Legal document classification (don't misclassify important docs)</li>
            </ul>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Goal:</strong> When you say "positive", be very confident</p>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--orange);">
            <h4 style="color: var(--orange);">High Recall Scenarios</h4>
            <p style="font-size: 8.5pt;"><strong>When False Negatives are costly:</strong></p>
            <ul style="font-size: 8pt;">
                <li>Disease detection (don't miss any sick patients)</li>
                <li>Fraud detection (catch all fraudulent transactions)</li>
                <li>Security threats (identify all potential threats)</li>
            </ul>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Goal:</strong> Catch all positive cases, even if some false alarms</p>
        </div>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ Metric Selection Guide</h4>
        <div class="decision-tree" style="font-size: 9pt;">
What's your class distribution?

BALANCED (40-60% split)
‚îî‚îÄ Use: <strong>Accuracy</strong>

IMBALANCED (&lt;20% or &gt;80%)
‚îú‚îÄ False Positives costly? ‚Üí <strong>Precision</strong>
‚îú‚îÄ False Negatives costly? ‚Üí <strong>Recall</strong>
‚îú‚îÄ Balance both? ‚Üí <strong>F1-Score</strong>
‚îî‚îÄ Need probabilities? ‚Üí <strong>PR-AUC</strong> (better than ROC-AUC)

MULTI-CLASS
‚îú‚îÄ Balanced ‚Üí <strong>Accuracy</strong>
‚îî‚îÄ Imbalanced ‚Üí <strong>Macro F1</strong> or <strong>Weighted F1</strong>
        </div>
    </div>
    
    <div class="page-number">Page 14</div>
</div>

<!-- PAGE 15: EVALUATION METRICS - REGRESSION -->
<div class="page">
    <h1>üìà Evaluation Metrics: Regression</h1>
    
    <h2>Regression Metrics Comparison</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th style="width: 15%;">Metric</th>
            <th style="width: 25%;">Formula</th>
            <th style="width: 30%;">When to Use</th>
            <th style="width: 15%;">Units</th>
            <th style="width: 15%;">Range</th>
        </tr>
        <tr>
            <td><strong>MAE</strong><br>Mean Absolute Error</td>
            <td>Œ£|y - ≈∑| / n</td>
            <td><strong>Outliers should not dominate</strong><br>Linear penalty for errors</td>
            <td>Same as target</td>
            <td>[0, ‚àû)<br>Lower better</td>
        </tr>
        <tr>
            <td><strong>MSE</strong><br>Mean Squared Error</td>
            <td>Œ£(y - ≈∑)¬≤ / n</td>
            <td><strong>Large errors very bad</strong><br>Penalizes large errors heavily</td>
            <td>Target¬≤</td>
            <td>[0, ‚àû)<br>Lower better</td>
        </tr>
        <tr>
            <td><strong>RMSE</strong><br>Root MSE</td>
            <td>‚àö(MSE)</td>
            <td><strong>Same units as target</strong><br>Interpretable, penalizes large errors</td>
            <td>Same as target</td>
            <td>[0, ‚àû)<br>Lower better</td>
        </tr>
        <tr>
            <td><strong>R¬≤</strong><br>R-squared</td>
            <td>1 - (SS_res/SS_tot)</td>
            <td><strong>% variance explained</strong><br>Model comparison</td>
            <td>None<br>(ratio)</td>
            <td>(-‚àû, 1]<br>1 = perfect</td>
        </tr>
        <tr>
            <td><strong>MAPE</strong><br>Mean Abs % Error</td>
            <td>Œ£|y-≈∑|/|y| / n √ó 100</td>
            <td><strong>Relative errors matter</strong><br>% error more meaningful</td>
            <td>Percentage</td>
            <td>[0, ‚àû)<br>Lower better</td>
        </tr>
    </table>
    
    <h2>Choosing the Right Metric</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">MAE (Mean Absolute Error)</h4>
            <p style="font-size: 8.5pt;"><strong>Use when:</strong><br>
            ‚Ä¢ Outliers present in data<br>
            ‚Ä¢ All errors equally important<br>
            ‚Ä¢ Want robust metric</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Example:</strong> Predicting house prices<br>
            MAE = $15,000 means avg error is $15K</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Pros:</strong> Easy to interpret, robust<br>
            <strong>Cons:</strong> Doesn't penalize large errors</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--teal);">RMSE (Root Mean Squared Error)</h4>
            <p style="font-size: 8.5pt;"><strong>Use when:</strong><br>
            ‚Ä¢ Large errors are very costly<br>
            ‚Ä¢ Want same units as target<br>
            ‚Ä¢ Standard regression metric</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Example:</strong> Predicting sales<br>
            RMSE = 100 units means typical error ~100</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Pros:</strong> Penalizes large errors, standard<br>
            <strong>Cons:</strong> Sensitive to outliers</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--purple);">R¬≤ (R-squared)</h4>
            <p style="font-size: 8.5pt;"><strong>Use when:</strong><br>
            ‚Ä¢ Want % variance explained<br>
            ‚Ä¢ Comparing multiple models<br>
            ‚Ä¢ Need interpretable metric</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Example:</strong> R¬≤ = 0.85<br>
            Model explains 85% of variance</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Pros:</strong> Scale-independent, interpretable<br>
            <strong>Cons:</strong> Can be misleading with non-linear data</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">MAPE (Mean Abs % Error)</h4>
            <p style="font-size: 8.5pt;"><strong>Use when:</strong><br>
            ‚Ä¢ Relative errors more meaningful<br>
            ‚Ä¢ Scale varies widely<br>
            ‚Ä¢ Business cares about %</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Example:</strong> MAPE = 5%<br>
            Predictions typically 5% off</p>
            <p style="font-size: 8.5pt; margin-top: 6pt;"><strong>Pros:</strong> % easy to communicate<br>
            <strong>Cons:</strong> Undefined when y=0, asymmetric</p>
        </div>
    </div>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è MAE vs MSE vs RMSE: Key Differences</h4>
        <table style="font-size: 9pt; margin-top: 6pt;">
            <tr>
                <th>Aspect</th>
                <th>MAE</th>
                <th>MSE</th>
                <th>RMSE</th>
            </tr>
            <tr>
                <td><strong>Outlier Sensitivity</strong></td>
                <td>Low (robust)</td>
                <td>High (sensitive)</td>
                <td>High (sensitive)</td>
            </tr>
            <tr>
                <td><strong>Large Error Penalty</strong></td>
                <td>Linear</td>
                <td>Quadratic (heavy)</td>
                <td>Quadratic (heavy)</td>
            </tr>
            <tr>
                <td><strong>Units</strong></td>
                <td>Same as target</td>
                <td>Target¬≤</td>
                <td>Same as target</td>
            </tr>
            <tr>
                <td><strong>Interpretability</strong></td>
                <td>High</td>
                <td>Low</td>
                <td>High</td>
            </tr>
            <tr>
                <td><strong>Typical Use</strong></td>
                <td>Robust models</td>
                <td>Loss function</td>
                <td>Model evaluation</td>
            </tr>
        </table>
    </div>
    
    <div class="box info">
        <h4>üìå Quick Decision Guide</h4>
        <div class="decision-tree" style="font-size: 9pt;">
What matters most?

OUTLIERS PRESENT
‚îî‚îÄ Use: <strong>MAE</strong> (robust to outliers)

LARGE ERRORS VERY COSTLY
‚îî‚îÄ Use: <strong>RMSE</strong> or <strong>MSE</strong> (penalizes heavily)

NEED INTERPRETABLE METRIC
‚îú‚îÄ Same units as target? ‚Üí <strong>RMSE</strong> or <strong>MAE</strong>
‚îî‚îÄ % variance explained? ‚Üí <strong>R¬≤</strong>

RELATIVE ERRORS MATTER (%, not absolute)
‚îî‚îÄ Use: <strong>MAPE</strong>

COMPARING MULTIPLE MODELS
‚îî‚îÄ Use: <strong>R¬≤</strong> (scale-independent)
        </div>
    </div>
    
    <div class="box tip">
        <h4>‚úÖ Regression Evaluation Best Practices</h4>
        <ul style="font-size: 9pt;">
            <li><strong>Use multiple metrics:</strong> RMSE + R¬≤ + MAE for complete picture</li>
            <li><strong>Plot residuals:</strong> Check for patterns (should be random)</li>
            <li><strong>Check residual distribution:</strong> Should be roughly normal</li>
            <li><strong>Look at prediction vs actual:</strong> Should follow diagonal line</li>
            <li><strong>Segment analysis:</strong> Check performance on different value ranges</li>
        </ul>
    </div>
    
    <div class="page-number">Page 15</div>
</div>

<!-- PAGE 16: MODEL COMPARISON MATRIX -->
<div class="page">
    <h1>‚öñÔ∏è Model Comparison Matrix</h1>
    
    <h2>Performance vs Complexity Tradeoff</h2>
    
    <table style="font-size: 7.5pt;">
        <tr>
            <th style="width: 12%;">Model</th>
            <th style="width: 10%;">Accuracy</th>
            <th style="width: 10%;">Train Time</th>
            <th style="width: 10%;">Inference</th>
            <th style="width: 12%;">Interpretability</th>
            <th style="width: 10%;">Tuning</th>
            <th style="width: 12%;">Handles Missing</th>
            <th style="width: 12%;">Scales to</th>
            <th style="width: 12%;">Best For</th>
        </tr>
        <tr>
            <td><strong>Logistic Reg</strong></td>
            <td>‚≠ê‚≠ê‚≠ê</td>
            <td>‚ö°‚ö°‚ö° Fast</td>
            <td>&lt;1ms</td>
            <td>‚úÖ‚úÖ‚úÖ High</td>
            <td>‚öôÔ∏è Minimal</td>
            <td>Need imputation</td>
            <td>100M rows</td>
            <td>Baseline, interpretable</td>
        </tr>
        <tr>
            <td><strong>Decision Tree</strong></td>
            <td>‚≠ê‚≠ê</td>
            <td>‚ö°‚ö°‚ö° Fast</td>
            <td>&lt;1ms</td>
            <td>‚úÖ‚úÖ‚úÖ High</td>
            <td>‚öôÔ∏è Minimal</td>
            <td>‚úÖ Native</td>
            <td>1M rows</td>
            <td>Interpretable, fast</td>
        </tr>
        <tr>
            <td><strong>Random Forest</strong></td>
            <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
            <td>‚ö°‚ö° Medium</td>
            <td>~10ms</td>
            <td>‚úÖ‚úÖ Medium</td>
            <td>‚öôÔ∏è Minimal</td>
            <td>‚úÖ Native</td>
            <td>10M rows</td>
            <td>Robust, good defaults</td>
        </tr>
        <tr style="background: #ffffcc;">
            <td><strong>XGBoost</strong></td>
            <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
            <td>‚ö°‚ö° Medium</td>
            <td>~10ms</td>
            <td>‚úÖ Low</td>
            <td>‚öôÔ∏è‚öôÔ∏è‚öôÔ∏è High</td>
            <td>‚úÖ Native</td>
            <td>100M rows</td>
            <td><strong>Winner for tabular</strong></td>
        </tr>
        <tr style="background: #ffffcc;">
            <td><strong>LightGBM</strong></td>
            <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
            <td>‚ö°‚ö°‚ö° Fast</td>
            <td>&lt;5ms</td>
            <td>‚úÖ Low</td>
            <td>‚öôÔ∏è‚öôÔ∏è‚öôÔ∏è High</td>
            <td>‚úÖ Native</td>
            <td>1B rows</td>
            <td><strong>Fastest, scales best</strong></td>
        </tr>
        <tr>
            <td><strong>SVM</strong></td>
            <td>‚≠ê‚≠ê‚≠ê</td>
            <td>‚ö° Slow</td>
            <td>~5ms</td>
            <td>‚úÖ Low</td>
            <td>‚öôÔ∏è‚öôÔ∏è Medium</td>
            <td>Need imputation</td>
            <td>100K rows</td>
            <td>Small datasets, kernels</td>
        </tr>
        <tr>
            <td><strong>K-NN</strong></td>
            <td>‚≠ê‚≠ê‚≠ê</td>
            <td>‚ö°‚ö°‚ö° Fast</td>
            <td>‚ö° Slow*</td>
            <td>‚úÖ‚úÖ Medium</td>
            <td>‚öôÔ∏è Minimal</td>
            <td>Need imputation</td>
            <td>1M rows</td>
            <td>Baseline, no training</td>
        </tr>
        <tr>
            <td><strong>Neural Net</strong></td>
            <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
            <td>‚ö° Slow‚Ä†</td>
            <td>~10ms</td>
            <td>‚ùå Very Low</td>
            <td>‚öôÔ∏è‚öôÔ∏è‚öôÔ∏è High</td>
            <td>Need imputation</td>
            <td>1B rows</td>
            <td>Images, text, audio</td>
        </tr>
        <tr>
            <td><strong>Naive Bayes</strong></td>
            <td>‚≠ê‚≠ê</td>
            <td>‚ö°‚ö°‚ö° Fast</td>
            <td>&lt;1ms</td>
            <td>‚úÖ‚úÖ Medium</td>
            <td>‚öôÔ∏è Minimal</td>
            <td>‚úÖ Handles well</td>
            <td>10M rows</td>
            <td>Text classification</td>
        </tr>
    </table>
    
    <p style="font-size: 8pt; margin-top: 6pt;">*K-NN slow at inference (must compute distance to all points)<br>
    ‚Ä†Neural nets fast with GPU, slow on CPU</p>
    
    <h2>When to Use Each Model</h2>
    
    <div class="grid-3" style="font-size: 8.5pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">Small Data (&lt;1K)</h4>
            <p style="font-size: 8pt;"><strong>1st:</strong> Logistic Regression<br>
            <strong>2nd:</strong> Random Forest<br>
            <strong>3rd:</strong> SVM</p>
            <p style="font-size: 8pt; margin-top: 6pt;"><strong>Avoid:</strong> Deep Learning (overfits)<br>
            XGBoost (needs tuning)</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--teal);">Medium (1K-100K)</h4>
            <p style="font-size: 8pt;"><strong>1st:</strong> XGBoost ‚≠ê<br>
            <strong>2nd:</strong> LightGBM<br>
            <strong>3rd:</strong> Random Forest</p>
            <p style="font-size: 8pt; margin-top: 6pt;"><strong>XGBoost wins 90%</strong><br>
            of Kaggle competitions</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">Large (100K+)</h4>
            <p style="font-size: 8pt;"><strong>1st:</strong> LightGBM ‚≠ê<br>
            <strong>2nd:</strong> XGBoost<br>
            <strong>3rd:</strong> Neural Nets</p>
            <p style="font-size: 8pt; margin-top: 6pt;"><strong>LightGBM fastest</strong><br>
            for tabular data</p>
        </div>
    </div>
    
    <div class="box warning" style="margin-top: 10pt;">
        <h4>‚ö†Ô∏è Model Selection Reality Check</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>For Tabular Data:</strong></p>
                <ul style="font-size: 8.5pt;">
                    <li>XGBoost/LightGBM beat neural nets 90% of time</li>
                    <li>Start with Random Forest (good defaults)</li>
                    <li>Then try XGBoost with tuning</li>
                    <li>Don't use deep learning unless 1M+ rows</li>
                </ul>
            </div>
            <div>
                <p><strong>For Unstructured Data:</strong></p>
                <ul style="font-size: 8.5pt;">
                    <li>Images: Always use CNNs (transfer learning)</li>
                    <li>Text: Use Transformers (BERT, GPT)</li>
                    <li>Audio: Use specialized architectures</li>
                    <li>Time Series: Try ARIMA/Prophet first</li>
                </ul>
            </div>
        </div>
    </div>
    
    <div class="page-number">Page 16</div>
</div>

<!-- PAGE 17: HYPERPARAMETER TUNING -->
<div class="page">
    <h1>‚öôÔ∏è Hyperparameter Tuning Guide</h1>
    
    <h2>Tuning Priority by Model</h2>
    
    <div class="grid-2" style="font-size: 8.5pt;">
        <div class="card" style="border-left: 4px solid var(--primary-blue);">
            <h4 style="color: var(--primary-blue);">Random Forest</h4>
            <pre style="font-size: 7.5pt;">
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100-500,    # More trees = better
    max_depth=None,          # Or 10-50
    min_samples_leaf=1-10,   # Higher = regularize
    max_features='sqrt',     # For classification
    n_jobs=-1                # Use all cores
)
            </pre>
            <p style="font-size: 8pt;"><strong>Priority:</strong> n_estimators, max_depth, min_samples_leaf</p>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--teal);">
            <h4 style="color: var(--teal);">XGBoost</h4>
            <pre style="font-size: 7.5pt;">
from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=100-1000,    # Tune with learning_rate
    learning_rate=0.01-0.3,   # Lower = more trees
    max_depth=3-10,           # Deeper = more complex
    subsample=0.5-1.0,        # Row sampling
    colsample_bytree=0.5-1.0, # Column sampling
    min_child_weight=1-10,    # Regularization
    gamma=0-5                 # Min loss reduction
)
            </pre>
            <p style="font-size: 8pt;"><strong>Priority:</strong> learning_rate + n_estimators, max_depth, subsample</p>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--orange);">
            <h4 style="color: var(--orange);">Neural Networks</h4>
            <pre style="font-size: 7.5pt;">
import torch.nn as nn
from torch.optim import Adam

model = nn.Sequential(
    nn.Linear(input_dim, 128),
    nn.ReLU(),
    nn.Dropout(0.2-0.5),     # Regularization
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Dropout(0.2-0.5),
    nn.Linear(64, output_dim)
)

optimizer = Adam(
    model.parameters(),
    lr=0.001,                # MOST IMPORTANT!
    weight_decay=1e-5        # L2 regularization
)
            </pre>
            <p style="font-size: 8pt;"><strong>Priority:</strong> learning_rate, batch_size, dropout</p>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--green);">
            <h4 style="color: var(--green);">LightGBM</h4>
            <pre style="font-size: 7.5pt;">
from lightgbm import LGBMClassifier

lgbm = LGBMClassifier(
    n_estimators=100-1000,
    learning_rate=0.01-0.3,
    max_depth=-1,             # No limit (or 3-10)
    num_leaves=31-255,        # Key parameter!
    subsample=0.5-1.0,
    colsample_bytree=0.5-1.0,
    min_child_samples=20      # Regularization
)
            </pre>
            <p style="font-size: 8pt;"><strong>Priority:</strong> num_leaves, learning_rate, max_depth</p>
        </div>
    </div>
    
    <h2>Tuning Methods Comparison</h2>
    
    <table style="font-size: 8.5pt;">
        <tr>
            <th style="width: 20%;">Method</th>
            <th style="width: 25%;">When to Use</th>
            <th style="width: 25%;">Pros</th>
            <th style="width: 30%;">Cons</th>
        </tr>
        <tr>
            <td><strong>Manual</strong></td>
            <td>Know domain, iterative</td>
            <td>Flexible, understand each parameter</td>
            <td>Time-consuming, may miss combinations</td>
        </tr>
        <tr>
            <td><strong>Grid Search</strong></td>
            <td>&lt;5 hyperparameters, fast training</td>
            <td>Exhaustive, finds best in grid</td>
            <td>Slow, combinatorial explosion</td>
        </tr>
        <tr>
            <td><strong>Random Search</strong></td>
            <td>Many hyperparameters</td>
            <td>Faster than grid, good coverage</td>
            <td>Random, not optimal</td>
        </tr>
        <tr>
            <td><strong>Optuna/Bayesian</strong></td>
            <td>Complex tuning, time available</td>
            <td>Smart search, best results</td>
            <td>Setup overhead, black box</td>
        </tr>
    </table>
    
    <div class="box info">
        <h4>üìå Quick Tuning Strategy</h4>
        <ol style="font-size: 9pt;">
            <li><strong>Start with defaults:</strong> Train baseline model</li>
            <li><strong>Tune most important params:</strong> Learning rate first!</li>
            <li><strong>Use validation set:</strong> Never tune on test set</li>
            <li><strong>Cross-validation:</strong> For robust estimates</li>
            <li><strong>Early stopping:</strong> Save time, prevent overfitting</li>
            <li><strong>Log everything:</strong> Track experiments</li>
        </ol>
    </div>
    
    <div class="box warning">
        <h4>‚ö†Ô∏è Hyperparameter Tuning Mistakes</h4>
        <div class="grid-2" style="font-size: 9pt;">
            <div>
                <p><strong>‚ùå Tuning on test set</strong><br>
                Always use validation or cross-validation</p>
                <p><strong>‚ùå Tuning too many params at once</strong><br>
                Tune most important first, then refine</p>
            </div>
            <div>
                <p><strong>‚ùå Not using early stopping</strong><br>
                Wastes time and can overfit</p>
                <p><strong>‚ùå Grid search for everything</strong><br>
                Use random search or Bayesian for many params</p>
            </div>
        </div>
    </div>
    
    <div class="page-number">Page 17</div>
</div>

<!-- PAGE 18: COMMON ALGORITHMS CODE -->
<div class="page">
    <h1>üíª Common Algorithms: Code Reference</h1>
    
    <h2>Classification</h2>
    
    <div class="grid-2" style="font-size: 8pt;">
        <div>
            <h4 style="color: var(--primary-blue);">Logistic Regression</h4>
            <pre style="font-size: 7pt;">
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Scale features (REQUIRED for linear models)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train model
lr = LogisticRegression(
    C=1.0,              # Inverse regularization (lower = more reg)
    penalty='l2',       # or 'l1' for feature selection
    max_iter=1000
)
lr.fit(X_train_scaled, y_train)

# Predict
y_pred = lr.predict(X_test_scaled)
y_prob = lr.predict_proba(X_test_scaled)
            </pre>
        </div>
        
        <div>
            <h4 style="color: var(--teal);">Random Forest</h4>
            <pre style="font-size: 7pt;">
from sklearn.ensemble import RandomForestClassifier

# NO SCALING NEEDED for trees!
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

# Predict
y_pred = rf.predict(X_test)

# Feature importance
import pandas as pd
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)
            </pre>
        </div>
        
        <div>
            <h4 style="color: var(--orange);">XGBoost</h4>
            <pre style="font-size: 7pt;">
from xgboost import XGBClassifier

# NO SCALING NEEDED
xgb = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    eval_metric='logloss'  # or 'auc', 'error'
)

# Train with validation for early stopping
xgb.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,
    verbose=False
)

# Predict
y_pred = xgb.predict(X_test)
            </pre>
        </div>
        
        <div>
            <h4 style="color: var(--green);">LightGBM</h4>
            <pre style="font-size: 7pt;">
from lightgbm import LGBMClassifier

# NO SCALING NEEDED
lgbm = LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    num_leaves=31,
    random_state=42
)

lgbm.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    callbacks=[lgbm.early_stopping(10)]
)

y_pred = lgbm.predict(X_test)
            </pre>
        </div>
    </div>
    
    <h2>Regression</h2>
    
    <div class="grid-2" style="font-size: 8pt;">
        <div>
            <h4 style="color: var(--purple);">Linear Regression</h4>
            <pre style="font-size: 7pt;">
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Ridge (L2) - default choice
ridge = Ridge(alpha=1.0)  # Higher alpha = more regularization
ridge.fit(X_train_scaled, y_train)

# Lasso (L1) - for feature selection
lasso = Lasso(alpha=0.1)
lasso.fit(X_train_scaled, y_train)

y_pred = ridge.predict(X_test_scaled)
            </pre>
        </div>
        
        <div>
            <h4 style="color: var(--teal);">XGBoost Regression</h4>
            <pre style="font-size: 7pt;">
from xgboost import XGBRegressor

xgb_reg = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    objective='reg:squarederror'
)

xgb_reg.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10
)

y_pred = xgb_reg.predict(X_test)
            </pre>
        </div>
    </div>
    
    <h2>Evaluation</h2>
    
    <pre style="font-size: 7.5pt;">
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import classification_report, confusion_matrix

# Classification metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# Regression metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R¬≤: {r2:.3f}")
    </pre>
    
    <div class="box tip">
        <h4>‚úÖ ML Workflow Template</h4>
        <pre style="font-size: 7pt;">
# 1. Import and load
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)
y = df['target']

# 2. Split (ALWAYS first!)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Feature engineering (fit on train only!)
# ... encoding, scaling, etc.

# 4. Train model
model.fit(X_train, y_train)

# 5. Evaluate
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")

# 6. Cross-validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score: {scores.mean():.3f} (+/- {scores.std():.3f})")
        </pre>
    </div>
    
    <div class="page-number">Page 18</div>
</div>

<!-- PAGE 19: QUICK REFERENCE SUMMARY -->
<div class="page">
    <h1>üìã Quick Reference Summary</h1>
    
    <h2>The ML Cheat Sheet</h2>
    
    <table style="font-size: 7.5pt;">
        <tr>
            <th style="width: 15%;">Scenario</th>
            <th style="width: 20%;">Data Type</th>
            <th style="width: 15%;">Sample Size</th>
            <th style="width: 20%;">Best Model</th>
            <th style="width: 15%;">Encoding</th>
            <th style="width: 15%;">Metric</th>
        </tr>
        <tr>
            <td>Customer churn</td>
            <td>Tabular</td>
            <td>10K-100K</td>
            <td>XGBoost</td>
            <td>Label/Target</td>
            <td>F1-Score</td>
        </tr>
        <tr>
            <td>House price</td>
            <td>Tabular</td>
            <td>1K-10K</td>
            <td>XGBoost</td>
            <td>Label/Target</td>
            <td>RMSE</td>
        </tr>
        <tr>
            <td>Image classification</td>
            <td>Images</td>
            <td>&lt;10K</td>
            <td>Transfer Learning (ResNet)</td>
            <td>N/A</td>
            <td>Accuracy</td>
        </tr>
        <tr>
            <td>Sentiment analysis</td>
            <td>Text</td>
            <td>1K-10K</td>
            <td>Fine-tune BERT</td>
            <td>Tokenization</td>
            <td>F1-Score</td>
        </tr>
        <tr>
            <td>Fraud detection</td>
            <td>Tabular</td>
            <td>100K+</td>
            <td>XGBoost + SMOTE</td>
            <td>Target</td>
            <td>PR-AUC</td>
        </tr>
        <tr>
            <td>Credit scoring</td>
            <td>Tabular</td>
            <td>10K+</td>
            <td>Logistic Reg (interpretable)</td>
            <td>One-hot</td>
            <td>ROC-AUC</td>
        </tr>
        <tr>
            <td>Time series forecasting</td>
            <td>Sequential</td>
            <td>&lt;1K</td>
            <td>ARIMA / Prophet</td>
            <td>N/A</td>
            <td>MAE</td>
        </tr>
        <tr>
            <td>Recommendation</td>
            <td>User-item</td>
            <td>100K+</td>
            <td>Collaborative Filtering</td>
            <td>Matrix factorization</td>
            <td>NDCG</td>
        </tr>
    </table>
    
    <h2>Golden Rules</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card" style="border-left: 4px solid var(--green);">
            <h4 style="color: var(--green);">‚úÖ DO This</h4>
            <ul style="font-size: 8.5pt; line-height: 1.5;">
                <li><strong>Start simple:</strong> Logistic Reg ‚Üí Random Forest ‚Üí XGBoost</li>
                <li><strong>Split first:</strong> Before any feature engineering</li>
                <li><strong>Cross-validate:</strong> Always use CV, not just train/test</li>
                <li><strong>Check assumptions:</strong> Normality, independence, etc.</li>
                <li><strong>Use right metric:</strong> F1 for imbalanced, RMSE for regression</li>
                <li><strong>Feature importance:</strong> Remove low-value features</li>
                <li><strong>Test significance:</strong> Ensure improvements are real</li>
                <li><strong>Document everything:</strong> Experiments, hyperparameters, results</li>
            </ul>
        </div>
        
        <div class="card" style="border-left: 4px solid var(--red);">
            <h4 style="color: var(--red);">‚ùå Don't Do This</h4>
            <ul style="font-size: 8.5pt; line-height: 1.5;">
                <li><strong>Data leakage:</strong> Never use test data during training</li>
                <li><strong>One-hot with trees:</strong> Use Label/Target encoding</li>
                <li><strong>Accuracy for imbalanced:</strong> Use F1 or PR-AUC</li>
                <li><strong>DL for tabular:</strong> XGBoost beats it 90% of time</li>
                <li><strong>Ignore overfitting:</strong> Monitor train vs val loss</li>
                <li><strong>Scale tree models:</strong> They don't need it!</li>
                <li><strong>Too many features:</strong> More features ‚â† better</li>
                <li><strong>No baseline:</strong> Always compare to simple model</li>
            </ul>
        </div>
    </div>
    
    <h2>Decision Framework Summary</h2>
    
    <div style="font-size: 9pt; background: var(--light-blue); padding: 12pt; border-radius: 6pt; border-left: 4px solid var(--primary-blue);">
        <p><strong>Step 1: Understand Problem</strong> ‚Üí Classification? Regression? Generation?</p>
        <p><strong>Step 2: Check Data Type</strong> ‚Üí Tabular? Images? Text? Time Series?</p>
        <p><strong>Step 3: Check Sample Size</strong> ‚Üí &lt;1K? 1K-10K? 10K-100K? 100K+?</p>
        <p><strong>Step 4: Choose Model</strong> ‚Üí Tabular‚ÜíXGBoost, Images‚ÜíCNN, Text‚ÜíBERT</p>
        <p><strong>Step 5: Encode Features</strong> ‚Üí Trees‚ÜíLabel, Linear‚ÜíOne-hot</p>
        <p><strong>Step 6: Scale if Needed</strong> ‚Üí Linear/NN‚ÜíYes, Trees‚ÜíNo</p>
        <p><strong>Step 7: Train & Validate</strong> ‚Üí Cross-validation, hyperparameter tuning</p>
        <p><strong>Step 8: Evaluate</strong> ‚Üí Right metric for problem, test significance</p>
        <p><strong>Step 9: Deploy & Monitor</strong> ‚Üí A/B test, track performance</p>
    </div>
    
    <h2>When in Doubt...</h2>
    
    <div class="grid-3" style="font-size: 9pt; margin-top: 10pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">For Tabular</h4>
            <p style="font-size: 8.5pt;">1. Random Forest (baseline)<br>
            2. XGBoost (tune it)<br>
            3. Done! (90% of time)</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">For Images</h4>
            <p style="font-size: 8.5pt;">1. Transfer Learning<br>
            2. Fine-tune ResNet/EfficientNet<br>
            3. Data augmentation</p>
        </div>
        
        <div class="card">
            <h4 style="color: var(--green);">For Text</h4>
            <p style="font-size: 8.5pt;">1. TF-IDF + LogReg (baseline)<br>
            2. Fine-tune BERT<br>
            3. Or use GPT API</p>
        </div>
    </div>
    
    <div class="page-number">Page 19</div>
</div>

<!-- PAGE 20: RESOURCES & FINAL TIPS -->
<div class="page">
    <h1>üìö Resources & Final Tips</h1>
    
    <h2>Learning Resources</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="card">
            <h4 style="color: var(--primary-blue);">üìñ Books</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>Hands-On Machine Learning</strong> (Aur√©lien G√©ron)<br>Best practical guide</li>
                <li><strong>The Elements of Statistical Learning</strong><br>Theory & foundations</li>
                <li><strong>Deep Learning</strong> (Goodfellow et al.)<br>DL bible</li>
                <li><strong>Pattern Recognition and ML</strong> (Bishop)<br>Comprehensive theory</li>
            </ul>
        </div>
        
        <div class="card">
            <h4 style="color: var(--teal);">üéì Courses</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>Fast.ai</strong><br>Practical deep learning</li>
                <li><strong>Coursera ML (Andrew Ng)</strong><br>Foundations</li>
                <li><strong>Stanford CS229</strong><br>ML theory</li>
                <li><strong>HuggingFace Course</strong><br>NLP & Transformers</li>
            </ul>
        </div>
        
        <div class="card">
            <h4 style="color: var(--orange);">üõ†Ô∏è Tools & Libraries</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>scikit-learn:</strong> Classical ML</li>
                <li><strong>XGBoost/LightGBM:</strong> Gradient boosting</li>
                <li><strong>PyTorch/TensorFlow:</strong> Deep learning</li>
                <li><strong>HuggingFace:</strong> Transformers</li>
                <li><strong>Optuna:</strong> Hyperparameter tuning</li>
            </ul>
        </div>
        
        <div class="card">
            <h4 style="color: var(--green);">üèÜ Practice</h4>
            <ul style="font-size: 8.5pt;">
                <li><strong>Kaggle:</strong> Competitions & datasets</li>
                <li><strong>UCI ML Repository:</strong> Classic datasets</li>
                <li><strong>Papers with Code:</strong> Latest research</li>
                <li><strong>GitHub:</strong> Open-source projects</li>
            </ul>
        </div>
    </div>
    
    <h2>Debugging Checklist</h2>
    
    <div class="grid-2" style="font-size: 9pt;">
        <div class="box danger">
            <h4>üêõ Model Not Learning?</h4>
            <ul style="font-size: 8.5pt;">
                <li>Check learning rate (too high/low?)</li>
                <li>Verify data preprocessing (normalized?)</li>
                <li>Check for data leakage</li>
                <li>Try simpler model first</li>
                <li>Visualize predictions vs actual</li>
                <li>Check for class imbalance</li>
            </ul>
        </div>
        
        <div class="box warning">
            <h4>‚ö†Ô∏è Overfitting?</h4>
            <ul style="font-size: 8.5pt;">
                <li>Train loss << Val loss?</li>
                <li>Add regularization (dropout, L2)</li>
                <li>Get more data</li>
                <li>Reduce model complexity</li>
                <li>Use cross-validation</li>
                <li>Early stopping</li>
            </ul>
        </div>
        
        <div class="box info">
            <h4>üìâ Underfitting?</h4>
            <ul style="font-size: 8.5pt;">
                <li>Train & val loss both high?</li>
                <li>Try more complex model</li>
                <li>Add more features</li>
                <li>Reduce regularization</li>
                <li>Train longer</li>
                <li>Check feature engineering</li>
            </ul>
        </div>
        
        <div class="box tip">
            <h4>‚úÖ Good Performance?</h4>
            <ul style="font-size: 8.5pt;">
                <li>Test on holdout set</li>
                <li>Check statistical significance</li>
                <li>Look at confusion matrix</li>
                <li>Analyze errors</li>
                <li>Test on edge cases</li>
                <li>A/B test in production</li>
            </ul>
        </div>
    </div>
    
    <h2>Final Words of Wisdom</h2>
    
    <div style="background: linear-gradient(135deg, var(--light-blue) 0%, white 100%); padding: 15pt; border-radius: 8pt; border: 2px solid var(--primary-blue); margin-top: 12pt;">
        <p style="font-size: 11pt; font-weight: 600; color: var(--primary-blue); margin-bottom: 8pt;">üéØ Remember:</p>
        <ul style="font-size: 9pt; line-height: 1.6;">
            <li><strong>Simple is better than complex.</strong> Start with simple models, add complexity only if needed.</li>
            <li><strong>Data > Algorithms.</strong> 100K samples with Random Forest beats 1K samples with deep learning.</li>
            <li><strong>Feature engineering matters.</strong> Good features with simple model beats bad features with complex model.</li>
            <li><strong>Always validate properly.</strong> Cross-validation is your friend. Test set is sacred.</li>
            <li><strong>XGBoost is king for tabular.</strong> Don't fight it. Use it.</li>
            <li><strong>Deep learning for unstructured.</strong> Images, text, audio ‚Üí use DL. Tabular ‚Üí use trees.</li>
            <li><strong>Metrics matter.</strong> Choose the right one. Accuracy lies on imbalanced data.</li>
            <li><strong>Iterate and experiment.</strong> ML is empirical. Try things, measure, improve.</li>
            <li><strong>Document everything.</strong> Future you will thank present you.</li>
            <li><strong>Keep learning.</strong> ML evolves fast. Stay curious! üöÄ</li>
        </ul>
    </div>
    
    <div style="text-align: center; margin-top: 20pt; padding: 15pt; background: var(--light-green); border-radius: 6pt;">
        <p style="font-size: 14pt; font-weight: 700; color: var(--green); margin-bottom: 6pt;">üéì You're Ready!</p>
        <p style="font-size: 10pt; color: var(--gray);">Keep this guide handy for your ML journey.<br>
        Laminate it, reference it, and keep building amazing models!</p>
        <p style="font-size: 9pt; color: var(--gray); margin-top: 10pt;"><em>Created October 2025 ‚Ä¢ Print-Ready ‚Ä¢ 8.5" x 11"</em></p>
    </div>
    
    <div class="page-number">Page 20</div>
</div>

</body>
</html>